---
title: "Use AI models in Compose | Docker Docs"
source_url: "https://docs.docker.com/ai/compose/models-and-compose/"
scraped_date: "2025-08-19T12:58:07.717012"
description: "Learn how to define and use AI models in Docker Compose applications using the models top-level element"
keywords: "compose,docker,compose,models,ai,machine,learning,cloud,providers,specification"
---
# Use AI models in Compose | Docker Docs

Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReferenceOpen sourceDocker Engine Install UbuntuDebianRHELFedoraRaspberry Pi OS (32-bit)CentOSSLES (s390x)BinariesPost-installation stepsStorage VolumesBind mountstmpfs mountsStorage drivers Select a storage driverBTRFS storage driverDevice Mapper storage driver (deprecated)OverlayFS storage driverVFS storage driverwindowsfilter storage driverZFS storage drivercontainerd image storeNetworking Packet filtering and firewallsNetwork drivers Bridge network driverHost network driverIPvlan network driverMacvlan network driverNone network driverOverlay network driver Tutorials Networking using a macvlan networkNetworking using the host networkNetworking with overlay networksNetworking with standalone containersCA certificatesLegacy container links Containers Start containers automaticallyRun multiple processes in a containerResource constraintsRuntime metricsRunning containers CLI CompletionProxy configurationFilter commandsFormat command and log outputOpenTelemetry for the Docker CLIDaemon Start the daemonUse IPv6 networkingDaemon proxy configurationLive restoreAlternative container runtimesCollect Docker metrics with PrometheusConfigure remote access for Docker daemonRead the daemon logsTroubleshooting the Docker daemon Manage resources Docker contextsDocker object labelsPrune unused Docker objectsLogs and metrics Configure logging driversCustomize log driver output Logging drivers Amazon CloudWatch Logs logging driverETW logging driverFluentd logging driverGoogle Cloud Logging driverGraylog Extended Format logging driverJournald logging driverJSON File logging driverLocal file logging driverSplunk logging driverSyslog logging driverUse a logging driver pluginUse docker logs with remote logging driversSecurity Rootless modeAntivirus software and DockerAppArmor security profiles for DockerContent trust in Docker Automation with content trustDelegations for content trustDeploy Notary Server with ComposeManage keys for content trustPlay in a content trust sandboxDocker security non-eventsIsolate containers with a user namespaceProtect the Docker daemon socketSeccomp security profiles for DockerVerify repository client with certificatesSwarm mode Administer and maintain a swarm of Docker EnginesDeploy a stack to a swarmDeploy services to a swarmGetting started with Swarm mode Create a swarmAdd nodes to the swarmDeploy a service to the swarmInspect a service on the swarmScale the service in the swarmDelete the service running on the swarmApply rolling updates to a serviceDrain a node on the swarm How swarm works How nodes workHow services workManage swarm security with public key infrastructure (PKI)Swarm task statesJoin nodes to a swarmLock your swarm to protect its encryption keyManage nodes in a swarmManage sensitive data with Docker secretsManage swarm service networksRaft consensus in swarm modeRun Docker Engine in swarm modeStore configuration data using Docker ConfigsSwarm mode key conceptsUse Swarm mode routing meshDeprecated featuresDocker Engine plugins Access authorization pluginDocker log driver pluginsDocker network driver pluginsDocker Plugin APIDocker volume pluginsPlugin Config Version 1 of Plugin V2Use Docker Engine plugins Release notes Engine v28Engine v27Engine v26.1Engine v26.0Engine v25.0Engine v24.0Engine v23.0Engine v20.10Engine v19.03Engine v18.09Engine v18.06Engine v18.05Engine v18.04Engine v18.03Engine v18.02Engine v18.01Engine v17.12Engine v17.11Engine v17.10Engine v17.09Engine v17.07Engine v17.06Engine v17.05Engine v17.04Engine v17.03Prior releasesDocker Build Core concepts Docker Build OverviewDockerfile overviewBuild context Building Multi-stageVariablesSecretsMulti-platformExport binariesContainer Device Interface (CDI)Best practicesBase imagesBuild checks NewBuilders Build drivers Docker container driverDocker driverKubernetes driverRemote driverManage buildersBake IntroductionTargetsInheritanceVariablesExpressionsFunctionsMatrix targetsContextsBake file referenceBake standard library functionsBuilding with Bake from a Compose fileOverriding configurationsRemote Bake file definitionCache Build cache invalidationBuild garbage collectionCache storage backends Amazon S3 cacheAzure Blob Storage cacheGitHub Actions cacheInline cacheLocal cacheRegistry cacheOptimize cache usage in buildsCI GitHub Actions AnnotationsAttestationsBuild checksBuild secretsBuild summaryBuildKit configurationCache managementCopy image between registriesExport to DockerLocal registryMulti-platform imageNamed contextsPush to multiple registriesReproducible buildsShare image between jobsTags and labelsTest before pushUpdate Docker Hub description Metadata AnnotationsBuild attestations Image attestation storageProvenance attestationsSBOM attestationsSLSA definitionsExporters Image and registry exportersLocal and tar exportersOCI and Docker exportersBuildKit buildkitd.tomlConfigure BuildKitCustom Dockerfile syntaxDockerfile release notes Debugging OpenTelemetry supportBuild release notesDocker Compose Introduction to Compose How Compose worksWhy use Compose?History and developmentInstall PluginStandaloneUninstallQuickstart How-tos Specify a project nameUse lifecycle hooksUse service profilesControl startup orderUse environment variables Set environment variablesEnvironment variables precedencePre-defined environment variablesInterpolationBest practicesBuild dependent imagesUse Compose WatchSecrets in ComposeNetworkingUse multiple Compose files MergeExtendIncludeEnable GPU supportUse Compose in productionOCI artifact applications NewUse provider services NewCompose Bridge UsageCustomize Support and feedback FAQsGive feedbackSample apps Releases Release notesMigrate to Compose v2MCP Gateway TestcontainersAIAsk Gordon Beta Model Context Protocol (MCP) Built-in tools in GordonConfigure MCP servers with YAMLDocker Model Runner Beta Get started with DMRDMR REST APIDMR examplesMCP Catalog and Toolkit Beta Docker Hub MCP serverDocker MCP CatalogMCP Toolkit AI and Docker Compose Use AI models in Compose NewProductsDocker Desktop Setup Install MacMac permission requirementsWindowsWindows permission requirementsLinux UbuntuDebianFedoraArchRHELVM or VDI environmentsSign inAllowlistExplore Docker Desktop ContainersImagesVolumesBuildsResource Saver modePause Docker Desktop Features and capabilities NetworkingGPU supportUSB/IP supportDeploy on KubernetesSynchronized file sharescontainerd image storeWasm workloads BetaDocker Desktop CLIVirtual Machine ManagerWSL Best practicesCustom kernels on WSLUse WSL Settings and maintenance Change settingsBackup and restore data Troubleshoot and support Troubleshoot and diagnose Common topicsKnown issuesGet support for Docker Desktop FAQs GeneralMacWindowsLinuxReleasesGive feedbackUninstallFix startup issue for MacRelease notesDocker Hardened Images New QuickstartAbout Hardened imagesImage typesImage testingResponsibility overviewFeatures FlexibilityContinuous patchingEnterprise supportHardened, secure imagesSeamless integrationHow-tos Explore imagesMirror an imageCustomize an imageUse an imageVerify an imageManage imagesScan an imageEnforce image usageMigrate an appDebug a containerCore concepts AttestationsCIS BenchmarkCode signingCVEsDistroless imagesFIPSglibc and muslHardeningImage digestsImage provenanceImmutabilitySBOMsSLSASoftware Supply Chain SecuritySSDLCSTIGVEXTroubleshootDocker Offload Beta QuickstartAboutConfigureUsage & billingOptimize usageTroubleshootGive feedbackDocker Build Cloud SetupUsageContinuous integrationOptimizationBuilder settingsRelease notesDocker Hub QuickstartLibrary SearchTrusted contentCatalogsMirrorRepositories Create Manage Repository informationAccessImages TagsImmutable tagsImage ManagementSoftware artifactsPush imagesMove imagesImage security insightsWebhooksAutomated builds Set upLink accountsAutomated repository testsAdvanced optionsManage autobuildsTroubleshootTrusted content Docker Official ImagesDocker Verified Publisher ProgramDocker-Sponsored Open Source ProgramInsights and analyticsArchiveDeletePersonal settingsUsage and limits PullsOptimize usageService accountsTroubleshootRelease notesDocker Scout InstallQuickstart Explore DashboardDocker Scout image analysisDocker Scout metrics exporterImage details viewManage vulnerability exceptions How-tos Create an exception using the GUICreate an exception using the VEXDocker Scout environment variablesDocker Scout SBOMsUse Scout with different artifact types Deep dive Advisory database sources and matching serviceData collection and storage in Docker ScoutPolicy Evaluation Configure policiesDocker Scout health scoresEvaluate policy compliance in CIRemediation with Docker ScoutView Docker Scout policy statusIntegrations Code quality SonarQube Container registries Amazon ECRAzure Container RegistryContinuous Integration Azure DevOps PipelinesCircle CIGitHub ActionsGitLab CI/CDJenkinsIntegrating Docker Scout with environments Generic (CLI)Sysdig Source code management GitHub Team collaboration Slack Release notes CLI release notesPlatform release notesDocker for GitHub Copilot EA InstallUsageExample promptsDocker Extensions Marketplace extensionsNon-marketplace extensionsConfigure a private marketplaceSettings and feedbackExtensions SDK The build and publish processQuickstart Part one: Build Create a simple extensionCreate an advanced frontend extensionAdd a backend to your extensionPart two: Publish Add labelsValidatePackage and release your extensionShare your extensionPublish in the MarketplaceBuild multi-arch extensionsArchitecture MetadataSecurityDesign and UI styling GuidelinesDocker design principlesMUI best practices Developer Guides AuthenticationInteracting with KubernetesInvoke host binariesUse the Docker socket Developer SDK tools Test and debugContinuous Integration (CI)CLI reference Extension APIs DashboardDockerExtension BackendExtension UI APINavigationTestcontainers CloudDeprecated products and featuresRelease lifecyclePlatformBilling Add or update a payment methodManage your billing information3D Secure authenticationView billing historyChange your billing cycleSubmit a tax exemption certificateFAQsDocker accounts AccountsCreate an accountManage an accountDeactivate an accountSecurity Personal access tokensTwo-factor authentication Recover your Docker account FAQs GeneralContainerNetwork and VM Single sign-on GeneralDomainsEnforcementIdentity providersUser managementSecurity announcementsSubscription Subscriptions and featuresSet up your subscriptionScale your subscriptionManage seatsChange your subscriptionDocker Desktop license agreementFAQsRelease notesEnterpriseAdministration Organization administration Create your organizationOnboard your organizationManage organization membersConvert an account into an organizationCreate and manage a teamDeactivate an organizationManage Docker productsActivity logsOrganization informationInsightsCompany administration overview Create a companyManage company membersManage company organizationsManage company owners FAQ OrganizationCompanyDeploy Docker Desktop MSI installerPKG installerMS StoreDeploy with IntuneDeploy with Jamf ProMicrosoft Dev BoxFAQsSecurity Single sign-on ConfigureConnectManageProvision Just-in-TimeSCIMGroup mappingEnforce sign-in ConfigureRoles and permissionsManage domainsHardened Docker Desktop Enhanced Container Isolation Enable ECIConfigure advanced settingsLimitationsFAQsSettings Management Use a JSON fileUse the Admin ConsoleDesktop settings reportingSettings referenceRegistry Access ManagementImage Access ManagementAir-gapped containersOrganization access tokens Troubleshoot Troubleshoot provisioningTroubleshoot SSOHome / Manuals / AI and Docker Compose / Use AI models in ComposeDefine AI Models in Docker Compose applicationsPage options Copy page as Markdown for LLMs View page as plain text Ask questions with Docs AI ClaudeOpen in ClaudeTable of contentsPrerequisitesWhat are Compose models?Basic model definitionModel configuration optionsAlternative configuration with provider servicesService model bindingShort syntaxLong syntaxPlatform portabilityDocker Model RunnerCloud providersCommon runtime configurationsDevelopmentConservative with disabled reasoningCreative with high randomnessHighly deterministicConcurrent processingRich vocabulary modelReferenceRequires: Docker Compose 2.38.0 and later Compose lets you define AI models as core components of your application, so you can declare model dependencies alongside services and run the application on any platform that supports the Compose Specification.PrerequisitesDocker Compose v2.38 or laterA platform that supports Compose models such as Docker Model Runner (DMR) or compatible cloud providers. If you are using DMR, see the requirements.What are Compose models?Compose models are a standardized way to define AI model dependencies in your application. By using the models top-level element in your Compose file, you can:Declare which AI models your application needsSpecify model configurations and requirementsMake your application portable across different platformsLet the platform handle model provisioning and lifecycle managementBasic model definitionTo define models in your Compose application, use the models top-level element: services: chat-app: image: my-chat-app models: - llm models: llm: model: ai/smollm2This example defines:A service called chat-app that uses a model named llmA model definition for llm that references the ai/smollm2 model imageModel configuration optionsModels support various configuration options: models: llm: model: ai/smollm2 context_size: 1024 runtime_flags: - "--a-flag" - "--another-flag=42"Common configuration options include:model (required): The OCI artifact identifier for the model. This is what Compose pulls and runs via the model runner.context_size: Defines the maximum token context size for the model. NoteEach model has its own maximum context size. When increasing the context length, consider your hardware constraints. In general, try to keep context size as small as feasible for your specific needs.runtime_flags: A list of raw command-line flags passed to the inference engine when the model is started. For example, if you use llama.cpp, you can pass any of the available parameters.Platform-specific options may also be available via extension attributes x-* TipSee more example in the Common runtime configurations section.Alternative configuration with provider services ImportantThis approach is deprecated. Use the models top-level element instead.You can also use the provider service type, which allows you to declare platform capabilities required by your application. For AI models, you can use the model type to declare model dependencies.To define a model provider: services: chat: image: my-chat-app depends_on: - ai_runner ai_runner: provider: type: model options: model: ai/smollm2 context-size: 1024 runtime-flags: "--no-prefill-assistant"Service model bindingServices can reference models in two ways: short syntax and long syntax.Short syntaxThe short syntax is the simplest way to bind a model to a service: services: app: image: my-app models: - llm - embedding-model models: llm: model: ai/smollm2 embedding-model: model: ai/all-minilmWith short syntax, the platform automatically generates environment variables based on the model name:LLM_URL - URL to access the LLM modelLLM_MODEL - Model identifier for the LLM modelEMBEDDING_MODEL_URL - URL to access the embedding-modelEMBEDDING_MODEL_MODEL - Model identifier for the embedding-modelLong syntaxThe long syntax allows you to customize environment variable names: services: app: image: my-app models: llm: endpoint_var: AI_MODEL_URL model_var: AI_MODEL_NAME embedding-model: endpoint_var: EMBEDDING_URL model_var: EMBEDDING_NAME models: llm: model: ai/smollm2 embedding-model: model: ai/all-minilmWith this configuration, your service receives:AI_MODEL_URL and AI_MODEL_NAME for the LLM modelEMBEDDING_URL and EMBEDDING_NAME for the embedding modelPlatform portabilityOne of the key benefits of using Compose models is portability across different platforms that support the Compose specification.Docker Model RunnerWhen Docker Model Runner is enabled: services: chat-app: image: my-chat-app models: llm: endpoint_var: AI_MODEL_URL model_var: AI_MODEL_NAME models: llm: model: ai/smollm2 context_size: 4096 runtime_flags: - "--no-prefill-assistant"Docker Model Runner will:Pull and run the specified model locallyProvide endpoint URLs for accessing the modelInject environment variables into the serviceCloud providersThe same Compose file can run on cloud providers that support Compose models: services: chat-app: image: my-chat-app models: - llm models: llm: model: ai/smollm2 # Cloud-specific configurations x-cloud-options: - "cloud.instance-type=gpu-small" - "cloud.region=us-west-2"Cloud providers might:Use managed AI services instead of running models locallyApply cloud-specific optimizations and scalingProvide additional monitoring and logging capabilitiesHandle model versioning and updates automaticallyCommon runtime configurationsBelow are some example configurations for various use cases.Development services: app: image: app models: dev_model: endpoint_var: DEV_URL model_var: DEV_MODEL models: dev_model: model: ai/model context_size: 4096 runtime_flags: - "--verbose" # Set verbosity level to infinity - "--verbose-prompt" # Print a verbose prompt before generation - "--log-prefix" # Enable prefix in log messages - "--log-timestamps" # Enable timestamps in log messages - "--log-colors" # Enable colored loggingConservative with disabled reasoning services: app: image: app models: conservative_model: endpoint_var: CONSERVATIVE_URL model_var: CONSERVATIVE_MODEL models: conservative_model: model: ai/model context_size: 4096 runtime_flags: - "--temp" # Temperature - "0.1" - "--top-k" # Top-k sampling - "1" - "--reasoning-budget" # Disable reasoning - "0"Creative with high randomness services: app: image: app models: creative_model: endpoint_var: CREATIVE_URL model_var: CREATIVE_MODEL models: creative_model: model: ai/model context_size: 4096 runtime_flags: - "--temp" # Temperature - "1" - "--top-p" # Top-p sampling - "0.9"Highly deterministic services: app: image: app models: deterministic_model: endpoint_var: DET_URL model_var: DET_MODEL models: deterministic_model: model: ai/model context_size: 4096 runtime_flags: - "--temp" # Temperature - "0" - "--top-k" # Top-k sampling - "1"Concurrent processing services: app: image: app models: concurrent_model: endpoint_var: CONCURRENT_URL model_var: CONCURRENT_MODEL models: concurrent_model: model: ai/model context_size: 2048 runtime_flags: - "--threads" # Number of threads to use during generation - "8" - "--mlock" # Lock memory to prevent swappingRich vocabulary model services: app: image: app models: rich_vocab_model: endpoint_var: RICH_VOCAB_URL model_var: RICH_VOCAB_MODEL models: rich_vocab_model: model: ai/model context_size: 4096 runtime_flags: - "--temp" # Temperature - "0.1" - "--top-p" # Top-p sampling - "0.9"Referencemodels top-level elementmodels attributeDocker Model Runner documentationCompose Model Runner documentation Edit this page Request changesTable of contentsPrerequisitesWhat are Compose models?Basic model definitionModel configuration optionsAlternative configuration with provider servicesService model bindingShort syntaxLong syntaxPlatform portabilityDocker Model RunnerCloud providersCommon runtime configurationsDevelopmentConservative with disabled reasoningCreative with high randomnessHighly deterministicConcurrent processingRich vocabulary modelReference