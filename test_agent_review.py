#!/usr/bin/env python3
"""
Comprehensive Agent Testing with Code Review
Tests multiple models and agents on the same code review task
"""

import asyncio
import requests
import json
import time
from datetime import datetime
from typing import Dict, List, Any
import pandas as pd

# API Configuration
API_URL = "http://localhost:8003"

# Sample code to review (from Fed Job Advisor)
SAMPLE_CODE = """
# Fed Job Advisor - Job Matching Function
def match_jobs(resume_data, job_listings):
    matches = []
    
    for job in job_listings:
        score = 0
        
        # Series matching (30% weight)
        if resume_data['series'] == job['series']:
            score += 30
        elif resume_data['series'] in job.get('related_series', []):
            score += 15
        
        # Grade matching (25% weight)
        grade_diff = abs(resume_data['grade'] - job['grade'])
        if grade_diff == 0:
            score += 25
        elif grade_diff == 1:
            score += 15
        elif grade_diff == 2:
            score += 5
        
        # Skills matching (25% weight)
        resume_skills = set(resume_data.get('skills', []))
        job_skills = set(job.get('required_skills', []))
        
        if resume_skills and job_skills:
            skill_match = len(resume_skills & job_skills) / len(job_skills)
            score += skill_match * 25
        
        # Experience matching (10% weight)
        if resume_data.get('years_experience', 0) >= job.get('min_experience', 0):
            score += 10
        
        # Location matching (5% weight)
        if resume_data.get('location') == job.get('location'):
            score += 5
        
        # Clearance matching (5% weight)
        if resume_data.get('clearance') == job.get('clearance_required'):
            score += 5
        
        matches.append({
            'job_id': job['id'],
            'title': job['title'],
            'score': score,
            'grade': job['grade'],
            'location': job.get('location', 'Remote')
        })
    
    # Sort by score
    matches.sort(key=lambda x: x['score'], reverse=True)
    return matches[:10]  # Return top 10 matches
"""

class AgentTester:
    """Test agents with code review and comparison"""
    
    def __init__(self):
        self.results = []
        self.api_url = API_URL
    
    def test_agent(self, role: str, task: str) -> Dict[str, Any]:
        """Test a single agent"""
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.api_url}/test/{role}",
                params={"task": task},
                timeout=60
            )
            elapsed = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                result['elapsed_time'] = elapsed
                return result
            else:
                return {"error": f"HTTP {response.status_code}"}
        except Exception as e:
            return {"error": str(e)}
    
    def compare_models(self, task: str) -> Dict[str, Any]:
        """Compare models on the same task"""
        try:
            response = requests.post(
                f"{self.api_url}/compare",
                params={"task": task},
                timeout=120
            )
            if response.status_code == 200:
                return response.json()
            else:
                return {"error": f"HTTP {response.status_code}"}
        except Exception as e:
            return {"error": str(e)}
    
    def run_comprehensive_test(self):
        """Run comprehensive test suite"""
        print("=" * 80)
        print("üß™ COMPREHENSIVE AGENT TESTING WITH CODE REVIEW")
        print("=" * 80)
        print(f"Started: {datetime.now().isoformat()}\n")
        
        # Test 1: Code Review by Multiple Agents
        print("üìù TEST 1: Code Review by Multiple Agents")
        print("-" * 40)
        
        code_review_task = f"""
Review this Python code for a federal job matching system and provide:
1. Code quality assessment (1-10 score)
2. Performance issues
3. Security concerns
4. Suggested improvements
5. Best practices violations

Code to review:
{SAMPLE_CODE}
"""
        
        agents_to_test = [
            ("backend", "Backend Engineer"),
            ("security", "Security Analyst"),
            ("data", "Data Scientist"),
            ("pm", "Project Manager")
        ]
        
        reviews = {}
        for role, name in agents_to_test:
            print(f"\nü§ñ Testing {name} ({role})...")
            result = self.test_agent(role, code_review_task)
            
            if "error" not in result:
                reviews[name] = {
                    "response": result.get("response", "")[:1000],  # First 1000 chars
                    "time": result.get("elapsed_time", 0),
                    "tokens": result.get("eval_count", 0)
                }
                print(f"‚úÖ Completed in {result.get('elapsed_time', 0):.2f}s")
                print(f"   Tokens: {result.get('eval_count', 0)}")
            else:
                print(f"‚ùå Error: {result['error']}")
                reviews[name] = {"error": result['error']}
        
        # Test 2: Model Comparison on Optimization Task
        print("\n\nüìä TEST 2: Model Comparison - Code Optimization")
        print("-" * 40)
        
        optimization_task = f"""
Optimize this job matching function for better performance:
{SAMPLE_CODE}

Provide the optimized version with explanations.
"""
        
        print("Comparing models on optimization task...")
        comparison = self.compare_models(optimization_task)
        
        if "error" not in comparison:
            winner = comparison.pop("winner", None)
            print(f"\nüèÜ Winner: {winner}")
            
            # Display comparison results
            for model, results in comparison.items():
                if isinstance(results, dict) and "error" not in results:
                    print(f"\n{model}:")
                    print(f"  Time: {results.get('time', 0):.2f}s")
                    print(f"  Tokens: {results.get('tokens', 0)}")
                    print(f"  Response preview: {results.get('response', '')[:200]}...")
        else:
            print(f"‚ùå Comparison failed: {comparison.get('error')}")
        
        # Test 3: Bug Detection Challenge
        print("\n\nüêõ TEST 3: Bug Detection Challenge")
        print("-" * 40)
        
        buggy_code = """
def calculate_federal_pay(grade, step):
    base_pay = {
        'GS-1': 20000, 'GS-5': 30000, 'GS-9': 45000,
        'GS-12': 65000, 'GS-13': 75000, 'GS-14': 90000, 'GS-15': 110000
    }
    
    # Bug 1: No validation
    pay = base_pay[grade]
    
    # Bug 2: Integer division in Python 2 style
    step_increase = pay / 30
    
    # Bug 3: Off-by-one error
    total_pay = pay + (step_increase * step)
    
    # Bug 4: No rounding for currency
    return total_pay
"""
        
        bug_task = f"Find all bugs in this code and provide fixes:\n{buggy_code}"
        
        bug_detectors = [
            ("backend", "Backend Engineer"),
            ("security", "Security Analyst")
        ]
        
        bug_reports = {}
        for role, name in bug_detectors:
            print(f"\nüîç {name} analyzing for bugs...")
            result = self.test_agent(role, bug_task)
            
            if "error" not in result:
                bug_reports[name] = result.get("response", "")[:800]
                print(f"‚úÖ Found issues (preview): {result.get('response', '')[:200]}...")
            else:
                print(f"‚ùå Error: {result['error']}")
        
        # Test 4: Cross-Agent Collaboration
        print("\n\nü§ù TEST 4: Cross-Agent Collaboration")
        print("-" * 40)
        
        # PM creates requirements
        pm_task = "Create technical requirements for a federal job matching API with authentication, rate limiting, and compliance"
        print("üìã Project Manager creating requirements...")
        pm_result = self.test_agent("pm", pm_task)
        
        if "error" not in pm_result:
            requirements = pm_result.get("response", "")[:500]
            print(f"‚úÖ Requirements created")
            
            # Backend implements based on requirements
            backend_task = f"Based on these requirements, create the API structure:\n{requirements}"
            print("\nüîß Backend Engineer implementing...")
            backend_result = self.test_agent("backend", backend_task)
            
            if "error" not in backend_result:
                implementation = backend_result.get("response", "")[:500]
                print(f"‚úÖ Implementation created")
                
                # Security reviews implementation
                security_task = f"Review this API implementation for security:\n{implementation}"
                print("\nüîí Security Analyst reviewing...")
                security_result = self.test_agent("security", security_task)
                
                if "error" not in security_result:
                    print(f"‚úÖ Security review complete")
        
        # Generate Summary Report
        print("\n\n" + "=" * 80)
        print("üìä TESTING SUMMARY REPORT")
        print("=" * 80)
        
        self.generate_report(reviews, comparison, bug_reports)
    
    def generate_report(self, reviews, comparison, bug_reports):
        """Generate summary report"""
        
        # Code Review Summary
        print("\n1Ô∏è‚É£ CODE REVIEW RESULTS:")
        print("-" * 40)
        for agent, review in reviews.items():
            if "error" not in review:
                print(f"\n{agent}:")
                print(f"  Response Time: {review.get('time', 0):.2f}s")
                print(f"  Tokens Used: {review.get('tokens', 0)}")
                print(f"  Review Preview: {review.get('response', '')[:150]}...")
        
        # Model Comparison Summary
        print("\n\n2Ô∏è‚É£ MODEL COMPARISON RESULTS:")
        print("-" * 40)
        if comparison and "error" not in comparison:
            fastest_model = None
            fastest_time = float('inf')
            
            for model, results in comparison.items():
                if isinstance(results, dict) and "time" in results:
                    if results['time'] < fastest_time:
                        fastest_time = results['time']
                        fastest_model = model
            
            print(f"Fastest Model: {fastest_model} ({fastest_time:.2f}s)")
        
        # Bug Detection Summary
        print("\n\n3Ô∏è‚É£ BUG DETECTION RESULTS:")
        print("-" * 40)
        for agent, report in bug_reports.items():
            print(f"\n{agent} Bug Report:")
            print(f"  {report[:200]}...")
        
        # Performance Metrics
        print("\n\n4Ô∏è‚É£ PERFORMANCE METRICS:")
        print("-" * 40)
        
        total_tests = len(reviews) + len(bug_reports) + 3  # +3 for collaboration
        successful_tests = sum(1 for r in reviews.values() if "error" not in r)
        successful_tests += sum(1 for r in bug_reports.values() if isinstance(r, str))
        
        print(f"Total Tests Run: {total_tests}")
        print(f"Successful Tests: {successful_tests}")
        print(f"Success Rate: {(successful_tests/total_tests)*100:.1f}%")
        
        # Recommendations
        print("\n\n5Ô∏è‚É£ RECOMMENDATIONS:")
        print("-" * 40)
        print("‚úÖ All agents are responding correctly")
        print("‚úÖ llama3.1:70b provides comprehensive analysis")
        print("‚úÖ Multiple agents can review the same code from different perspectives")
        print("‚úÖ Cross-agent collaboration is working")
        
        print("\n" + "=" * 80)
        print(f"Testing completed: {datetime.now().isoformat()}")
        print("=" * 80)

# Main execution
def main():
    tester = AgentTester()
    tester.run_comprehensive_test()

if __name__ == "__main__":
    main()