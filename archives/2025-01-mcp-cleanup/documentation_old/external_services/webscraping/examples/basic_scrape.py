#!/usr/bin/env python3
"""
Basic webscraping examples using the WebscrapingSpecialist
"""

import asyncio
import json
from mcp_services.external.webscraping_specialist import WebscrapingSpecialist

async def example_single_page_scrape():
    """Example: Scrape a single documentation page"""
    
    scraper = WebscrapingSpecialist()
    
    # Scrape Python documentation homepage
    result = await scraper.scrape_single_page(
        url="https://docs.python.org/3/",
        extract_links=True,
        content_selector="main"  # Target main content area
    )
    
    if result["success"]:
        print(f"âœ… Successfully scraped: {result['title']}")
        print(f"ğŸ“„ Content length: {len(result['content'])} characters")
        print(f"ğŸ”— Found {len(result['links'])} links")
        print(f"ğŸ“Š Metadata: {json.dumps(result['metadata'], indent=2)}")
        
        # Show first 200 characters of content
        print(f"ğŸ“ Content preview: {result['content'][:200]}...")
        
        # Show first 5 links
        print(f"ğŸ”— Link examples:")
        for link in result['links'][:5]:
            print(f"   - {link}")
    else:
        print(f"âŒ Scraping failed: {result['error']}")

async def example_documentation_traversal():
    """Example: Traverse a documentation site"""
    
    scraper = WebscrapingSpecialist()
    
    # Traverse FastAPI documentation (limited scope)
    result = await scraper.traverse_documentation(
        start_url="https://fastapi.tiangolo.com/",
        max_depth=2,           # Only go 2 levels deep
        max_pages=10,          # Limit to 10 pages for this example
        link_patterns=[        # Only follow tutorial and guide links
            r"/tutorial/",
            r"/guide/",
            r"/getting-started/"
        ]
    )
    
    if result["success"]:
        print(f"âœ… Successfully traversed documentation")
        print(f"ğŸ“š Total pages scraped: {result['total_pages']}")
        print(f"ğŸ Starting URL: {result['start_url']}")
        print(f"âš™ï¸ Configuration: {json.dumps(result['configuration'], indent=2)}")
        
        # Show all scraped URLs
        print(f"ğŸ“„ Scraped pages:")
        for url, page_data in result['content'].items():
            print(f"   - {page_data['title']} ({url})")
        
        # Show any errors encountered
        if result['errors']:
            print(f"âš ï¸ Errors encountered: {len(result['errors'])}")
            for error in result['errors']:
                print(f"   - {error['url']}: {error['error']}")
    else:
        print(f"âŒ Traversal failed: {result['error']}")

async def example_custom_content_extraction():
    """Example: Custom content extraction with specific selectors"""
    
    scraper = WebscrapingSpecialist()
    
    # Scrape GitHub README with custom selector
    result = await scraper.scrape_single_page(
        url="https://github.com/python/cpython",
        extract_links=False,  # Don't need links for this example
        content_selector="article.markdown-body"  # GitHub README selector
    )
    
    if result["success"]:
        print(f"âœ… Custom extraction successful")
        print(f"ğŸ“„ Title: {result['title']}")
        print(f"ğŸ“Š Headings found: {len(result['metadata']['headings'])}")
        
        # Show heading structure
        print(f"ğŸ“‹ Document structure:")
        for heading in result['metadata']['headings'][:10]:  # First 10 headings
            indent = "  " * (heading['level'] - 1)
            print(f"{indent}H{heading['level']}: {heading['text']}")
        
        # Show content preview
        print(f"ğŸ“ Content preview: {result['content'][:300]}...")
    else:
        print(f"âŒ Custom extraction failed: {result['error']}")

async def main():
    """Run all examples"""
    
    print("ğŸ•·ï¸ Webscraping Specialist Examples")
    print("=" * 50)
    
    print("\n1ï¸âƒ£ Single Page Scraping Example")
    print("-" * 30)
    await example_single_page_scrape()
    
    print("\n2ï¸âƒ£ Documentation Traversal Example")  
    print("-" * 30)
    await example_documentation_traversal()
    
    print("\n3ï¸âƒ£ Custom Content Extraction Example")
    print("-" * 30)
    await example_custom_content_extraction()
    
    print("\nâœ… All examples completed!")

if __name__ == "__main__":
    # Run the examples
    asyncio.run(main())