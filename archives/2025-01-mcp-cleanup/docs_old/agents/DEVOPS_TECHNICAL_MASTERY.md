# DevOps Engineer Agent - Technical Mastery Knowledge Base

**Version**: 1.0  
**Date**: August 19, 2025  
**Purpose**: Technical expertise for DevOps Engineer MCP agent to research and provide DevOps implementation guidance  
**Usage**: Knowledge base for researching DevOps technologies and providing technical implementation prompts  

---

## ðŸŽ¯ **TECHNICAL MASTERY: DevOps Implementation Expertise**

### **Containerization and Orchestration**

#### **Docker Mastery**
```yaml
Docker Architecture:
  Core Components:
    Docker Engine: "Runtime that manages containers, images, networks, and volumes"
    Docker Daemon (dockerd): "Background process that manages Docker objects"
    Docker CLI: "Command-line interface for interacting with Docker daemon"
    Docker Registry: "Storage and distribution system for Docker images"
    
  Container Lifecycle:
    Image Creation: "docker build from Dockerfile"
    Container Run: "docker run creates and starts container from image"
    Container States: "Created, Running, Paused, Stopped, Exited"
    Container Removal: "docker rm removes container, docker rmi removes image"

Advanced Docker Features:
  Multi-stage Builds:
    Purpose: "Reduce image size by using multiple FROM statements"
    Pattern: "Build stage + production stage"
    
    Example Dockerfile:
      # Multi-stage build for Node.js application
      FROM node:18-alpine AS builder
      WORKDIR /app
      COPY package*.json ./
      RUN npm ci --only=production
      
      FROM node:18-alpine AS production
      RUN addgroup -g 1001 -S nodejs && adduser -S nextjs -u 1001
      WORKDIR /app
      COPY --from=builder /app/node_modules ./node_modules
      COPY --chown=nextjs:nodejs . .
      USER nextjs
      EXPOSE 3000
      CMD ["node", "server.js"]
      
  Docker Networking:
    Bridge Network: "Default network for containers on single host"
    Host Network: "Container shares host's network stack"
    Overlay Network: "Multi-host networking for swarm mode"
    Custom Networks: "User-defined networks with DNS resolution"
    
    Network Commands:
      # Create custom network
      docker network create --driver bridge app-network
      
      # Run container in specific network
      docker run -d --name web --network app-network nginx
      
      # Connect existing container to network
      docker network connect app-network database
      
  Volume Management:
    Named Volumes: "Docker-managed persistent storage"
    Bind Mounts: "Host directory mounted into container"
    tmpfs Mounts: "In-memory filesystem for temporary data"
    
    Volume Examples:
      # Named volume for database data
      docker run -d --name postgres \
        -v postgres_data:/var/lib/postgresql/data \
        postgres:13
        
      # Bind mount for development
      docker run -d --name web \
        -v $(pwd):/app \
        -p 3000:3000 \
        node:18-alpine

Container Security:
  Image Security:
    Base Image Selection: "Use official, minimal images (alpine, distroless)"
    Vulnerability Scanning: "docker scan, trivy, clair"
    Image Signing: "Docker Content Trust, Cosign"
    
  Runtime Security:
    Non-root User: "Create and use non-privileged user in container"
    Read-only Filesystem: "--read-only flag with tmpfs for writable areas"
    Resource Limits: "CPU, memory, and I/O constraints"
    Security Contexts: "AppArmor, SELinux profiles"
    
    Security Example:
      # Secure container run
      docker run -d \
        --name secure-app \
        --user 1001:1001 \
        --read-only \
        --tmpfs /tmp \
        --memory="512m" \
        --cpus="1.0" \
        --security-opt no-new-privileges \
        my-app:latest
```

#### **Kubernetes Mastery**
```yaml
Kubernetes Architecture:
  Control Plane Components:
    kube-apiserver: "Frontend to Kubernetes, handles REST operations"
    etcd: "Distributed key-value store for cluster data"
    kube-scheduler: "Assigns pods to nodes based on resource requirements"
    kube-controller-manager: "Runs controllers that regulate cluster state"
    
  Node Components:
    kubelet: "Node agent that manages pods and containers"
    kube-proxy: "Network proxy implementing service concept"
    Container Runtime: "Docker, containerd, or CRI-O"
    
Core Kubernetes Objects:
  Pod:
    Definition: "Smallest deployable unit, one or more containers"
    Lifecycle: "Pending -> Running -> Succeeded/Failed"
    Networking: "Containers share network namespace and storage"
    
    Pod Specification:
      apiVersion: v1
      kind: Pod
      metadata:
        name: web-pod
        labels:
          app: web
      spec:
        containers:
        - name: web
          image: nginx:1.21
          ports:
          - containerPort: 80
          resources:
            requests:
              memory: "64Mi"
              cpu: "250m"
            limits:
              memory: "128Mi"
              cpu: "500m"
              
  Deployment:
    Purpose: "Manages ReplicaSets and provides declarative updates"
    Rolling Updates: "Zero-downtime updates with configurable strategy"
    Rollback: "Revert to previous revision if issues occur"
    
    Deployment Example:
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: web-deployment
      spec:
        replicas: 3
        strategy:
          type: RollingUpdate
          rollingUpdate:
            maxSurge: 1
            maxUnavailable: 1
        selector:
          matchLabels:
            app: web
        template:
          metadata:
            labels:
              app: web
          spec:
            containers:
            - name: web
              image: nginx:1.21
              ports:
              - containerPort: 80

  Service:
    Types: "ClusterIP, NodePort, LoadBalancer, ExternalName"
    Service Discovery: "DNS-based service discovery within cluster"
    Load Balancing: "Distributes traffic across healthy pods"
    
    Service Types:
      # ClusterIP (internal access only)
      apiVersion: v1
      kind: Service
      metadata:
        name: web-service
      spec:
        selector:
          app: web
        ports:
        - port: 80
          targetPort: 80
        type: ClusterIP
        
      # LoadBalancer (external access)
      apiVersion: v1
      kind: Service
      metadata:
        name: web-service-public
      spec:
        selector:
          app: web
        ports:
        - port: 80
          targetPort: 80
        type: LoadBalancer

Advanced Kubernetes Concepts:
  ConfigMaps and Secrets:
    ConfigMap: "Non-confidential configuration data"
    Secret: "Sensitive data like passwords, tokens, keys"
    
    Usage Examples:
      # ConfigMap creation
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: app-config
      data:
        database_url: "postgresql://db:5432/myapp"
        log_level: "info"
        
      # Secret creation
      apiVersion: v1
      kind: Secret
      metadata:
        name: app-secret
      type: Opaque
      data:
        username: YWRtaW4=  # base64 encoded
        password: MWYyZDFlMmU2N2Rm  # base64 encoded
        
      # Using ConfigMap and Secret in Pod
      apiVersion: v1
      kind: Pod
      spec:
        containers:
        - name: app
          image: myapp:latest
          env:
          - name: DATABASE_URL
            valueFrom:
              configMapKeyRef:
                name: app-config
                key: database_url
          - name: DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: app-secret
                key: password

  Persistent Volumes:
    PersistentVolume (PV): "Storage resource provisioned by administrator"
    PersistentVolumeClaim (PVC): "Storage request by user"
    StorageClass: "Dynamic provisioning of storage"
    
    Storage Example:
      # PersistentVolumeClaim
      apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: postgres-pvc
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: fast-ssd
        
      # Using PVC in Deployment
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: postgres
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: postgres
        template:
          metadata:
            labels:
              app: postgres
          spec:
            containers:
            - name: postgres
              image: postgres:13
              volumeMounts:
              - name: postgres-storage
                mountPath: /var/lib/postgresql/data
            volumes:
            - name: postgres-storage
              persistentVolumeClaim:
                claimName: postgres-pvc

Kubernetes Networking:
  CNI (Container Network Interface):
    Flannel: "Simple overlay network using VXLAN"
    Calico: "Layer 3 networking with network policies"
    Weave Net: "Mesh networking with automatic discovery"
    Cilium: "eBPF-based networking with advanced security"
    
  Network Policies:
    Purpose: "Control traffic flow between pods"
    Default Deny: "Block all traffic, then allow specific connections"
    
    Network Policy Example:
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: deny-all
      spec:
        podSelector: {}
        policyTypes:
        - Ingress
        - Egress
        
      ---
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-web-to-db
      spec:
        podSelector:
          matchLabels:
            app: database
        policyTypes:
        - Ingress
        ingress:
        - from:
          - podSelector:
              matchLabels:
                app: web
          ports:
          - protocol: TCP
            port: 5432
```

### **CI/CD Pipeline Implementation**

#### **Jenkins Mastery**
```yaml
Jenkins Architecture:
  Master Node:
    Responsibilities: "Scheduling builds, dispatching to agents, monitoring"
    Components: "Web UI, REST API, job configuration storage"
    Plugins: "Extensible architecture with thousands of plugins"
    
  Agent Nodes:
    Types: "Static agents, dynamic agents (cloud), Docker agents"
    Communication: "JNLP, SSH, or direct connection to master"
    Labels: "Categorize agents for specific job requirements"

Pipeline as Code:
  Declarative Pipeline:
    Structure: "pipeline { stages { stage { steps } } }"
    Benefits: "Version controlled, reproducible, visual pipeline view"
    
    Declarative Example:
      pipeline {
          agent any
          
          environment {
              NODE_VERSION = '18'
              DOCKER_REGISTRY = 'my-registry.com'
          }
          
          stages {
              stage('Checkout') {
                  steps {
                      git branch: 'main', url: 'https://github.com/company/app.git'
                  }
              }
              
              stage('Build') {
                  steps {
                      sh '''
                          npm ci
                          npm run build
                      '''
                  }
              }
              
              stage('Test') {
                  parallel {
                      stage('Unit Tests') {
                          steps {
                              sh 'npm run test:unit'
                          }
                          post {
                              always {
                                  publishTestResults testResultsPattern: 'test-results.xml'
                              }
                          }
                      }
                      
                      stage('Integration Tests') {
                          steps {
                              sh 'npm run test:integration'
                          }
                      }
                      
                      stage('Security Scan') {
                          steps {
                              sh 'npm audit --audit-level high'
                          }
                      }
                  }
              }
              
              stage('Docker Build') {
                  steps {
                      script {
                          def image = docker.build("${DOCKER_REGISTRY}/myapp:${BUILD_NUMBER}")
                          docker.withRegistry("https://${DOCKER_REGISTRY}", 'registry-credentials') {
                              image.push()
                              image.push('latest')
                          }
                      }
                  }
              }
              
              stage('Deploy to Staging') {
                  steps {
                      script {
                          kubernetesDeploy(
                              configs: 'k8s/staging/*.yaml',
                              kubeconfigId: 'staging-kubeconfig'
                          )
                      }
                  }
              }
              
              stage('Deploy to Production') {
                  when {
                      branch 'main'
                  }
                  steps {
                      input message: 'Deploy to production?', ok: 'Deploy',
                            submitterParameter: 'DEPLOYER'
                      script {
                          kubernetesDeploy(
                              configs: 'k8s/production/*.yaml',
                              kubeconfigId: 'production-kubeconfig'
                          )
                      }
                  }
              }
          }
          
          post {
              always {
                  cleanWs()
              }
              failure {
                  emailext (
                      subject: "Build Failed: ${currentBuild.fullDisplayName}",
                      body: "Build failed. Please check: ${BUILD_URL}",
                      to: "${env.CHANGE_AUTHOR_EMAIL}"
                  )
              }
              success {
                  slackSend (
                      channel: '#deployments',
                      color: 'good',
                      message: "Deployed ${env.JOB_NAME} build ${env.BUILD_NUMBER} to production"
                  )
              }
          }
      }

Jenkins Plugins and Integration:
  Essential Plugins:
    Pipeline: "Pipeline as code functionality"
    Docker: "Docker build and publish capabilities"
    Kubernetes: "Deploy to Kubernetes clusters"
    Git: "Source code management integration"
    Blue Ocean: "Modern, visual pipeline interface"
    
  Security Plugins:
    OWASP Dependency Check: "Vulnerability scanning for dependencies"
    SonarQube Scanner: "Code quality and security analysis"
    Credentials Plugin: "Secure credential management"
    
  Testing and Quality:
    JUnit: "Test result publishing and trending"
    Coverage: "Code coverage reporting"
    Performance: "Performance test result analysis"

Distributed Builds:
  Agent Configuration:
    Static Agents: "Permanently connected build nodes"
    Cloud Agents: "Dynamic provisioning (AWS, Azure, GCP)"
    Docker Agents: "Containerized build environments"
    
    Agent Setup Example:
      # Docker agent template
      pipeline {
          agent {
              docker {
                  image 'node:18-alpine'
                  args '-v /var/run/docker.sock:/var/run/docker.sock'
              }
          }
          stages {
              stage('Build') {
                  steps {
                      sh 'npm ci && npm run build'
                  }
              }
          }
      }
```

#### **GitLab CI/CD Mastery**
```yaml
GitLab CI Architecture:
  Components:
    GitLab Instance: "Web interface, Git repository, CI/CD orchestration"
    GitLab Runner: "Agent that executes CI/CD jobs"
    Executor Types: "Shell, Docker, Kubernetes, VirtualBox, SSH"
    
Pipeline Configuration:
  .gitlab-ci.yml Structure:
    stages: "Define pipeline stages"
    jobs: "Define what to execute in each stage"
    variables: "Environment variables for pipeline"
    before_script/after_script: "Common setup/cleanup commands"
    
  Advanced GitLab CI Example:
    # .gitlab-ci.yml
    stages:
      - build
      - test
      - security
      - deploy-staging
      - deploy-production
    
    variables:
      DOCKER_DRIVER: overlay2
      DOCKER_TLS_CERTDIR: "/certs"
      REGISTRY_URL: $CI_REGISTRY_IMAGE
      
    before_script:
      - echo "Pipeline started for commit $CI_COMMIT_SHA"
      
    # Build stage
    build:
      stage: build
      image: docker:20.10.16
      services:
        - docker:20.10.16-dind
      variables:
        IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
      script:
        - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
        - docker build -t $IMAGE_TAG .
        - docker push $IMAGE_TAG
      only:
        - main
        - develop
        - merge_requests
        
    # Test stage with parallel jobs
    test:unit:
      stage: test
      image: node:18-alpine
      cache:
        key: $CI_COMMIT_REF_SLUG
        paths:
          - node_modules/
      script:
        - npm ci
        - npm run test:unit -- --coverage
      artifacts:
        reports:
          coverage: coverage/clover.xml
          junit: test-results.xml
        expire_in: 1 week
        
    test:integration:
      stage: test
      image: docker:20.10.16
      services:
        - docker:20.10.16-dind
        - postgres:13
        - redis:6-alpine
      variables:
        POSTGRES_DB: testdb
        POSTGRES_USER: testuser
        POSTGRES_PASSWORD: testpass
        DATABASE_URL: postgresql://testuser:testpass@postgres:5432/testdb
        REDIS_URL: redis://redis:6379
      script:
        - docker build -t test-image .
        - docker run --rm --network host -e DATABASE_URL -e REDIS_URL test-image npm run test:integration
        
    # Security scanning
    security:sast:
      stage: security
      image: docker:20.10.16
      services:
        - docker:20.10.16-dind
      script:
        - docker run --rm -v $(pwd):/app -w /app securecodewarrior/docker-scout:latest
      artifacts:
        reports:
          sast: security-report.json
      allow_failure: true
      
    security:dependency:
      stage: security
      image: node:18-alpine
      script:
        - npm audit --audit-level high
        - npm install -g retire
        - retire --path . --outputformat json --outputpath retire-report.json
      artifacts:
        reports:
          dependency_scanning: retire-report.json
      allow_failure: true
      
    # Deployment stages
    deploy:staging:
      stage: deploy-staging
      image: bitnami/kubectl:latest
      environment:
        name: staging
        url: https://staging.myapp.com
      script:
        - kubectl config use-context staging
        - sed -i "s/IMAGE_TAG/$CI_COMMIT_SHA/g" k8s/staging/*.yaml
        - kubectl apply -f k8s/staging/
        - kubectl rollout status deployment/myapp -n staging --timeout=300s
      only:
        - main
        - develop
        
    deploy:production:
      stage: deploy-production
      image: bitnami/kubectl:latest
      environment:
        name: production
        url: https://myapp.com
      script:
        - kubectl config use-context production
        - sed -i "s/IMAGE_TAG/$CI_COMMIT_SHA/g" k8s/production/*.yaml
        - kubectl apply -f k8s/production/
        - kubectl rollout status deployment/myapp -n production --timeout=600s
      when: manual
      only:
        - main
      dependencies:
        - deploy:staging

GitLab Runner Configuration:
  Docker Executor:
    Benefits: "Isolated builds, consistent environments, easy scaling"
    Configuration: "config.toml with Docker-specific settings"
    
    Runner Config Example:
      # /etc/gitlab-runner/config.toml
      [[runners]]
        name = "docker-runner"
        url = "https://gitlab.com/"
        token = "RUNNER_TOKEN"
        executor = "docker"
        [runners.docker]
          image = "alpine:latest"
          privileged = true
          volumes = ["/var/run/docker.sock:/var/run/docker.sock", "/cache"]
          pull_policy = "if-not-present"
          
  Kubernetes Executor:
    Benefits: "Scalable, resource-efficient, automated cleanup"
    Configuration: "Kubernetes namespace and resource specifications"
    
    K8s Runner Example:
      # values.yaml for GitLab Runner Helm chart
      runners:
        config: |
          [[runners]]
            [runners.kubernetes]
              namespace = "gitlab-runner"
              image = "alpine:latest"
              privileged = true
              [[runners.kubernetes.volumes.empty_dir]]
                name = "docker-certs"
                mount_path = "/certs/client"
                medium = "Memory"
```

#### **GitHub Actions Mastery**
```yaml
GitHub Actions Architecture:
  Components:
    Workflow: "YAML file defining automated processes"
    Job: "Set of steps executed on same runner"
    Step: "Individual task (action or shell command)"
    Runner: "Virtual machine that executes workflows"
    
  Workflow Triggers:
    Push: "Code pushed to repository"
    Pull Request: "PR opened, updated, or closed"
    Schedule: "Cron-based scheduling"
    Workflow Dispatch: "Manual trigger with inputs"
    Repository Dispatch: "External trigger via API"

Advanced GitHub Actions:
  Matrix Strategy:
    Purpose: "Run jobs across multiple configurations"
    Usage: "Test across multiple OS, Node versions, etc."
    
    Matrix Example:
      # .github/workflows/ci.yml
      name: CI
      
      on:
        push:
          branches: [main, develop]
        pull_request:
          branches: [main]
          
      jobs:
        test:
          runs-on: ${{ matrix.os }}
          strategy:
            matrix:
              os: [ubuntu-latest, windows-latest, macos-latest]
              node-version: [16, 18, 20]
              include:
                - os: ubuntu-latest
                  node-version: 18
                  coverage: true
          
          steps:
            - uses: actions/checkout@v4
            
            - name: Setup Node.js ${{ matrix.node-version }}
              uses: actions/setup-node@v4
              with:
                node-version: ${{ matrix.node-version }}
                cache: 'npm'
                
            - name: Install dependencies
              run: npm ci
              
            - name: Run tests
              run: npm test
              
            - name: Upload coverage reports
              if: matrix.coverage
              uses: codecov/codecov-action@v3
              with:
                files: ./coverage/lcov.info
                
  Custom Actions:
    Types: "JavaScript actions, Docker container actions, Composite actions"
    Marketplace: "Reusable actions shared with community"
    
    Composite Action Example:
      # .github/actions/deploy/action.yml
      name: 'Deploy Application'
      description: 'Deploy application to Kubernetes'
      inputs:
        environment:
          description: 'Deployment environment'
          required: true
        image-tag:
          description: 'Docker image tag'
          required: true
        kubeconfig:
          description: 'Kubernetes config'
          required: true
          
      runs:
        using: 'composite'
        steps:
          - name: Setup kubectl
            uses: azure/setup-kubectl@v3
            with:
              version: 'v1.24.0'
              
          - name: Configure kubectl
            shell: bash
            run: |
              echo "${{ inputs.kubeconfig }}" | base64 -d > kubeconfig
              export KUBECONFIG=kubeconfig
              
          - name: Deploy
            shell: bash
            run: |
              sed -i 's/IMAGE_TAG/${{ inputs.image-tag }}/g' k8s/${{ inputs.environment }}/*.yaml
              kubectl apply -f k8s/${{ inputs.environment }}/
              kubectl rollout status deployment/myapp -n ${{ inputs.environment }}

  Secrets and Security:
    Repository Secrets: "Encrypted environment variables"
    Organization Secrets: "Shared across repositories"
    Environment Secrets: "Specific to deployment environments"
    
    Security Best Practices:
      - Use least privilege principle
      - Rotate secrets regularly  
      - Use environment protection rules
      - Audit secret usage
      
    Security Example:
      name: Secure Deployment
      
      jobs:
        deploy:
          runs-on: ubuntu-latest
          environment: production
          steps:
            - uses: actions/checkout@v4
            
            - name: Deploy to production
              env:
                AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
                AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                KUBECONFIG_DATA: ${{ secrets.PROD_KUBECONFIG }}
              run: |
                echo "$KUBECONFIG_DATA" | base64 -d > kubeconfig
                export KUBECONFIG=kubeconfig
                kubectl apply -f k8s/production/
```

### **Infrastructure as Code (IaC)**

#### **Terraform Mastery**
```yaml
Terraform Core Concepts:
  Providers: "Plugins that interact with APIs (AWS, Azure, GCP, Kubernetes)"
  Resources: "Infrastructure components managed by Terraform"
  Data Sources: "Read information from existing infrastructure"
  Variables: "Input parameters for configurations"
  Outputs: "Return values from configurations"
  
  State Management:
    Local State: "terraform.tfstate file stored locally"
    Remote State: "State stored in S3, Azure Storage, or Terraform Cloud"
    State Locking: "Prevents concurrent modifications"
    
    Remote State Configuration:
      # backend.tf
      terraform {
        backend "s3" {
          bucket         = "mycompany-terraform-state"
          key            = "infrastructure/terraform.tfstate"
          region         = "us-west-2"
          encrypt        = true
          dynamodb_table = "terraform-state-lock"
        }
        
        required_providers {
          aws = {
            source  = "hashicorp/aws"
            version = "~> 5.0"
          }
          kubernetes = {
            source  = "hashicorp/kubernetes"
            version = "~> 2.0"
          }
        }
      }

Advanced Terraform Patterns:
  Modules:
    Purpose: "Reusable, configurable infrastructure components"
    Structure: "inputs (variables), outputs, resources"
    
    Module Example:
      # modules/vpc/main.tf
      resource "aws_vpc" "main" {
        cidr_block           = var.cidr_block
        enable_dns_hostnames = var.enable_dns_hostnames
        enable_dns_support   = var.enable_dns_support
        
        tags = merge(
          var.tags,
          {
            Name = var.name
          }
        )
      }
      
      resource "aws_subnet" "private" {
        count = length(var.private_subnets)
        
        vpc_id            = aws_vpc.main.id
        cidr_block        = var.private_subnets[count.index]
        availability_zone = data.aws_availability_zones.available.names[count.index]
        
        tags = merge(
          var.tags,
          {
            Name = "${var.name}-private-${count.index + 1}"
            Type = "private"
          }
        )
      }
      
      # modules/vpc/variables.tf
      variable "name" {
        description = "Name prefix for VPC resources"
        type        = string
      }
      
      variable "cidr_block" {
        description = "CIDR block for VPC"
        type        = string
        validation {
          condition     = can(cidrhost(var.cidr_block, 0))
          error_message = "The cidr_block value must be a valid CIDR block."
        }
      }
      
      variable "private_subnets" {
        description = "List of private subnet CIDR blocks"
        type        = list(string)
        default     = []
      }
      
      # modules/vpc/outputs.tf
      output "vpc_id" {
        description = "ID of the VPC"
        value       = aws_vpc.main.id
      }
      
      output "private_subnet_ids" {
        description = "IDs of the private subnets"
        value       = aws_subnet.private[*].id
      }
      
  Workspaces:
    Purpose: "Manage multiple environments with same configuration"
    Usage: "terraform workspace new staging"
    
    Workspace Usage:
      # main.tf with workspace-aware configuration
      locals {
        environment = terraform.workspace
        
        instance_counts = {
          default = 1
          staging = 2
          production = 5
        }
        
        instance_types = {
          default = "t3.micro"
          staging = "t3.small"  
          production = "t3.medium"
        }
      }
      
      resource "aws_instance" "web" {
        count         = local.instance_counts[local.environment]
        ami           = data.aws_ami.ubuntu.id
        instance_type = local.instance_types[local.environment]
        
        tags = {
          Name        = "${local.environment}-web-${count.index + 1}"
          Environment = local.environment
        }
      }

Terraform Best Practices:
  Code Organization:
    File Structure: "main.tf, variables.tf, outputs.tf, versions.tf"
    Environment Separation: "Separate directories or workspaces"
    Module Usage: "DRY principle with reusable modules"
    
  Security Practices:
    Sensitive Variables: "Mark variables as sensitive"
    State Encryption: "Encrypt state files"
    IAM Policies: "Use least privilege access"
    
    Security Example:
      variable "database_password" {
        description = "Password for database"
        type        = string
        sensitive   = true
      }
      
      resource "aws_db_instance" "main" {
        password = var.database_password
        # Other configuration...
        
        # Prevent accidental deletion
        deletion_protection = true
        skip_final_snapshot = false
        
        lifecycle {
          prevent_destroy = true
        }
      }

  Testing and Validation:
    Validation Rules: "Built-in variable validation"
    Plan Review: "Always review terraform plan before apply"
    Automated Testing: "Terratest, Kitchen-Terraform"
    
    Testing Example:
      # test/terraform_test.go
      package test
      
      import (
        "testing"
        "github.com/gruntwork-io/terratest/modules/terraform"
        "github.com/stretchr/testify/assert"
      )
      
      func TestTerraformVPCModule(t *testing.T) {
        terraformOptions := &terraform.Options{
          TerraformDir: "../modules/vpc",
          Vars: map[string]interface{}{
            "name":       "test-vpc",
            "cidr_block": "10.0.0.0/16",
          },
        }
        
        defer terraform.Destroy(t, terraformOptions)
        terraform.InitAndApply(t, terraformOptions)
        
        vpcId := terraform.Output(t, terraformOptions, "vpc_id")
        assert.NotEmpty(t, vpcId)
      }
```

#### **Ansible Mastery**
```yaml
Ansible Architecture:
  Control Node: "Machine running Ansible commands"
  Managed Nodes: "Target systems configured by Ansible"
  Inventory: "List of managed nodes with connection details"
  Playbooks: "YAML files defining automation tasks"
  Modules: "Units of code executed on managed nodes"
  
Ansible Inventory:
  Static Inventory:
    INI Format: "Traditional configuration file format"
    YAML Format: "Structured inventory with groups and variables"
    
    YAML Inventory Example:
      # inventory.yml
      all:
        children:
          webservers:
            hosts:
              web1:
                ansible_host: 10.0.1.10
                ansible_user: ubuntu
              web2:
                ansible_host: 10.0.1.11
                ansible_user: ubuntu
            vars:
              http_port: 80
              max_clients: 200
              
          databases:
            hosts:
              db1:
                ansible_host: 10.0.2.10
                ansible_user: postgres
            vars:
              postgresql_version: 13
              max_connections: 100
              
          production:
            children:
              webservers:
              databases:
            vars:
              environment: production
              backup_enabled: true
              
  Dynamic Inventory:
    Cloud Integration: "AWS, Azure, GCP dynamic inventory plugins"
    Custom Scripts: "Generate inventory from external sources"
    
    AWS Dynamic Inventory:
      # aws_ec2.yml
      plugin: aws_ec2
      regions:
        - us-west-2
        - us-east-1
      keyed_groups:
        - key: tags
          prefix: tag
        - key: instance_type
          prefix: instance_type
      compose:
        ansible_host: public_ip_address

Advanced Playbooks:
  Complex Playbook Structure:
    # site.yml - Main orchestration playbook
    ---
    - name: Configure web servers
      hosts: webservers
      become: yes
      roles:
        - common
        - nginx
        - ssl-certificates
      vars:
        ssl_enabled: true
        
    - name: Configure databases
      hosts: databases
      become: yes
      roles:
        - common
        - postgresql
        - backup-agent
      vars:
        postgresql_max_connections: "{{ max_connections }}"
        
    - name: Deploy application
      hosts: webservers
      become: yes
      serial: 1  # Rolling deployment
      tasks:
        - name: Stop application service
          service:
            name: myapp
            state: stopped
            
        - name: Update application code
          git:
            repo: https://github.com/company/myapp.git
            dest: /opt/myapp
            version: "{{ app_version | default('main') }}"
          notify:
            - restart application
            
        - name: Install/update dependencies
          pip:
            requirements: /opt/myapp/requirements.txt
            virtualenv: /opt/myapp/venv
            
        - name: Start application service
          service:
            name: myapp
            state: started
            enabled: yes
            
        - name: Wait for application to be ready
          uri:
            url: "http://{{ ansible_host }}/health"
            status_code: 200
          retries: 30
          delay: 10
          
      handlers:
        - name: restart application
          service:
            name: myapp
            state: restarted

Ansible Roles:
  Role Structure:
    tasks/: "Main automation tasks"
    handlers/: "Tasks triggered by notify"
    templates/: "Jinja2 template files"
    files/: "Static files to copy"
    vars/: "Role-specific variables"
    defaults/: "Default variable values"
    meta/: "Role metadata and dependencies"
    
  Example Role (nginx):
    # roles/nginx/tasks/main.yml
    ---
    - name: Install nginx
      package:
        name: nginx
        state: present
        
    - name: Create nginx configuration
      template:
        src: nginx.conf.j2
        dest: /etc/nginx/nginx.conf
        backup: yes
      notify:
        - restart nginx
        
    - name: Create virtual host configuration
      template:
        src: vhost.conf.j2
        dest: "/etc/nginx/sites-available/{{ item.name }}"
      loop: "{{ nginx_vhosts }}"
      notify:
        - restart nginx
        
    - name: Enable virtual hosts
      file:
        src: "/etc/nginx/sites-available/{{ item.name }}"
        dest: "/etc/nginx/sites-enabled/{{ item.name }}"
        state: link
      loop: "{{ nginx_vhosts }}"
      notify:
        - restart nginx
        
    - name: Start and enable nginx
      service:
        name: nginx
        state: started
        enabled: yes
        
    # roles/nginx/handlers/main.yml
    ---
    - name: restart nginx
      service:
        name: nginx
        state: restarted
        
    # roles/nginx/templates/nginx.conf.j2
    user nginx;
    worker_processes {{ ansible_processor_vcpus }};
    
    error_log /var/log/nginx/error.log;
    pid /run/nginx.pid;
    
    events {
        worker_connections {{ nginx_worker_connections | default(1024) }};
    }
    
    http {
        log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                        '$status $body_bytes_sent "$http_referer" '
                        '"$http_user_agent" "$http_x_forwarded_for"';
                        
        access_log /var/log/nginx/access.log main;
        
        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;
        
        include /etc/nginx/mime.types;
        default_type application/octet-stream;
        
        include /etc/nginx/sites-enabled/*;
    }

Ansible Vault:
  Purpose: "Encrypt sensitive data in playbooks"
  Usage: "ansible-vault create/edit/encrypt/decrypt"
  
  Vault Example:
    # Create encrypted file
    ansible-vault create group_vars/production/vault.yml
    
    # Content (encrypted):
    $ANSIBLE_VAULT;1.1;AES256
    66386439...encrypted content...
    
    # Use in playbook:
    - name: Set database password
      mysql_user:
        name: "{{ db_user }}"
        password: "{{ vault_db_password }}"
        
    # Run with vault password:
    ansible-playbook -i inventory.yml site.yml --ask-vault-pass
```

### **Monitoring and Observability**

#### **Prometheus and Grafana Mastery**
```yaml
Prometheus Architecture:
  Components:
    Prometheus Server: "Time series database and query engine"
    Exporters: "Collect metrics from applications and systems"
    Pushgateway: "Receive metrics from batch jobs"
    Alertmanager: "Handle alerts and notifications"
    
  Data Model:
    Metrics: "Time series identified by name and labels"
    Labels: "Key-value pairs for dimensionality"
    Samples: "Timestamp and numeric value"
    
    Metric Types:
      Counter: "Monotonically increasing value"
      Gauge: "Value that can go up or down"
      Histogram: "Distribution of observations"
      Summary: "Similar to histogram with quantiles"

Prometheus Configuration:
  prometheus.yml Example:
    # prometheus.yml
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      
    rule_files:
      - "rules/*.yml"
      
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
              
    scrape_configs:
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
          
      - job_name: 'node-exporter'
        static_configs:
          - targets: 
            - 'web1:9100'
            - 'web2:9100'
            - 'db1:9100'
        scrape_interval: 10s
        metrics_path: /metrics
        
      - job_name: 'application'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: 
                - production
                - staging
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
            
Custom Metrics and Exporters:
  Application Metrics:
    # Python Flask example with prometheus_client
    from prometheus_client import Counter, Histogram, Gauge, generate_latest
    import time
    
    # Define metrics
    REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
    REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration', ['method', 'endpoint'])
    ACTIVE_CONNECTIONS = Gauge('active_connections', 'Active database connections')
    
    @app.route('/metrics')
    def metrics():
        return generate_latest()
        
    @app.before_request
    def before_request():
        request.start_time = time.time()
        
    @app.after_request
    def after_request(response):
        duration = time.time() - request.start_time
        REQUEST_COUNT.labels(request.method, request.endpoint, response.status_code).inc()
        REQUEST_DURATION.labels(request.method, request.endpoint).observe(duration)
        return response

PromQL (Prometheus Query Language):
  Basic Queries:
    Instant Vector: "http_requests_total"
    Range Vector: "http_requests_total[5m]"
    Scalar: "scalar(http_requests_total)"
    
  Functions and Operators:
    Rate: "rate(http_requests_total[5m])"  # Per-second rate
    Sum: "sum(http_requests_total) by (instance)"
    Increase: "increase(http_requests_total[1h])"
    
  Advanced Queries:
    # 95th percentile response time
    histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
    
    # Error rate percentage
    (
      sum(rate(http_requests_total{status=~"5.."}[5m])) /
      sum(rate(http_requests_total[5m]))
    ) * 100
    
    # Top 10 endpoints by request rate
    topk(10, sum(rate(http_requests_total[5m])) by (endpoint))

Alerting Rules:
  # rules/application.yml
  groups:
    - name: application_alerts
      rules:
        - alert: HighErrorRate
          expr: |
            (
              sum(rate(http_requests_total{status=~"5.."}[5m])) /
              sum(rate(http_requests_total[5m]))
            ) * 100 > 5
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High error rate detected"
            description: "Error rate is {{ $value }}% for the last 5 minutes"
            
        - alert: HighMemoryUsage
          expr: |
            (
              node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
            ) / node_memory_MemTotal_bytes * 100 > 85
          for: 2m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
          annotations:
            summary: "High memory usage on {{ $labels.instance }}"
            description: "Memory usage is {{ $value }}%"

Grafana Dashboard Configuration:
  Dashboard as Code:
    # dashboard.json (simplified)
    {
      "dashboard": {
        "title": "Application Dashboard",
        "panels": [
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total[5m])) by (instance)",
                "legendFormat": "{{ instance }}"
              }
            ],
            "yAxes": [
              {
                "label": "Requests/sec"
              }
            ]
          },
          {
            "title": "Response Time",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
                "legendFormat": "95th percentile"
              },
              {
                "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
                "legendFormat": "Median"
              }
            ]
          }
        ]
      }
    }
```

---

## ðŸŽ¯ **Agent Implementation Guidance**

### **How This Technical Mastery Enhances Agent Performance**

#### **DevOps Technology Research**
- **Container Expertise**: Deep knowledge of Docker, Kubernetes, and container orchestration patterns
- **CI/CD Mastery**: Comprehensive understanding of Jenkins, GitLab CI, and GitHub Actions implementations
- **Infrastructure as Code**: Expert knowledge of Terraform, Ansible, and infrastructure automation
- **Monitoring and Observability**: Advanced techniques for Prometheus, Grafana, and system monitoring

#### **Problem-Solving Approach**
- **Technology Selection**: Expert guidance on choosing appropriate DevOps tools for specific scenarios
- **Architecture Design**: Advanced patterns for scalable, reliable DevOps infrastructure
- **Performance Optimization**: Techniques for optimizing CI/CD pipelines and deployment processes
- **Security Integration**: DevSecOps practices and security automation in deployment pipelines

### **Agent Usage Instructions**

#### **When to Apply This Technical Knowledge**
```python
# Example usage in agent decision-making
if deployment_challenge == "microservices" and platform == "kubernetes":
    recommend_kubernetes_deployment_patterns()
    suggest_helm_chart_structure()
    provide_service_mesh_considerations()
    
if ci_cd_requirement == "multi_environment" and tool == "jenkins":
    recommend_pipeline_as_code_approach()
    suggest_environment_promotion_strategy()
    provide_security_scanning_integration()
    
if infrastructure_need == "cloud_agnostic" and approach == "iac":
    compare_terraform_vs_ansible()
    recommend_multi_cloud_strategy()
    provide_state_management_best_practices()
```

#### **Research Output Enhancement**
All DevOps agent research should include:
- **Technology-specific implementations** with detailed code examples and configurations
- **Architecture patterns and best practices** for scalable and reliable systems
- **Security considerations** with DevSecOps integration strategies
- **Performance optimization techniques** with specific tuning recommendations
- **Monitoring and observability solutions** with comprehensive instrumentation guidance

---

*This technical mastery knowledge base transforms the DevOps Engineer Agent from general DevOps guidance to deep technical expertise, enabling sophisticated research and implementation recommendations for containerization, CI/CD, infrastructure automation, and system reliability challenges.*

**Â© 2025 Fed Job Advisor - DevOps Engineer Agent Technical Mastery Enhancement**