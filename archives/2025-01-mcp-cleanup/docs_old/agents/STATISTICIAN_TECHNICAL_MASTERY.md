# Statistician Agent - Technical Mastery Knowledge Base

**Version**: 1.0  
**Date**: January 19, 2025  
**Purpose**: Technical expertise for Statistician MCP agent to research and provide statistical implementation guidance  
**Usage**: Knowledge base for researching statistical methods and providing mathematical implementation prompts  

---

## üéØ **TECHNICAL MASTERY: Statistical Analysis Implementation Expertise**

### **Probability Theory and Distributions**

#### **Probability Foundations**
```yaml
Probability Axioms and Rules:
  Kolmogorov Axioms:
    "P(A) ‚â• 0 for any event A"
    "P(Œ©) = 1 where Œ© is the sample space"
    "P(A‚ÇÅ ‚à™ A‚ÇÇ ‚à™ ...) = P(A‚ÇÅ) + P(A‚ÇÇ) + ... for mutually exclusive events"
    
  Fundamental Rules:
    Addition Rule: "P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B)"
    Multiplication Rule: "P(A ‚à© B) = P(A|B) √ó P(B) = P(B|A) √ó P(A)"
    Conditional Probability: "P(A|B) = P(A ‚à© B) / P(B), provided P(B) > 0"
    Law of Total Probability: "P(A) = Œ£ P(A|B·µ¢) √ó P(B·µ¢)"
    Bayes' Theorem: "P(A|B) = P(B|A) √ó P(A) / P(B)"

Independence and Dependence:
  Statistical Independence:
    Definition: "P(A ‚à© B) = P(A) √ó P(B)"
    Equivalent: "P(A|B) = P(A) and P(B|A) = P(B)"
    
  Conditional Independence:
    Definition: "P(A ‚à© B|C) = P(A|C) √ó P(B|C)"
    Applications: "Naive Bayes assumptions, Markov models"

Random Variables:
  Discrete Random Variables:
    Probability Mass Function: "P(X = x) for discrete values"
    Cumulative Distribution Function: "F(x) = P(X ‚â§ x)"
    Expected Value: "E[X] = Œ£ x √ó P(X = x)"
    Variance: "Var(X) = E[X¬≤] - (E[X])¬≤"
    
  Continuous Random Variables:
    Probability Density Function: "f(x) where P(a < X < b) = ‚à´‚Çê·µá f(x) dx"
    Properties: "f(x) ‚â• 0 and ‚à´‚Çã‚àû^‚àû f(x) dx = 1"
    Expected Value: "E[X] = ‚à´‚Çã‚àû^‚àû x f(x) dx"
    Variance: "Var(X) = ‚à´‚Çã‚àû^‚àû (x - Œº)¬≤ f(x) dx"
```

#### **Common Probability Distributions**
```yaml
Discrete Distributions:
  Bernoulli Distribution:
    Parameters: "p (probability of success)"
    PMF: "P(X = 1) = p, P(X = 0) = 1-p"
    Mean: "p"
    Variance: "p(1-p)"
    Implementation: "scipy.stats.bernoulli"
    
  Binomial Distribution:
    Parameters: "n (trials), p (probability of success)"
    PMF: "P(X = k) = C(n,k) √ó p^k √ó (1-p)^(n-k)"
    Mean: "np"
    Variance: "np(1-p)"
    Implementation: "scipy.stats.binom"
    Use Cases: "Quality control, A/B testing, clinical trials"
    
  Poisson Distribution:
    Parameter: "Œª (rate parameter)"
    PMF: "P(X = k) = e^(-Œª) √ó Œª^k / k!"
    Mean: "Œª"
    Variance: "Œª"
    Implementation: "scipy.stats.poisson"
    Use Cases: "Count data, rare events, queueing theory"
    
  Negative Binomial Distribution:
    Parameters: "r (successes), p (probability)"
    PMF: "P(X = k) = C(k+r-1, k) √ó p^r √ó (1-p)^k"
    Mean: "r(1-p)/p"
    Variance: "r(1-p)/p¬≤"
    Implementation: "scipy.stats.nbinom"
    Use Cases: "Overdispersed count data, epidemiology"

Continuous Distributions:
  Normal Distribution:
    Parameters: "Œº (mean), œÉ¬≤ (variance)"
    PDF: "f(x) = (1/œÉ‚àö2œÄ) √ó e^(-(x-Œº)¬≤/2œÉ¬≤)"
    Properties: "Symmetric, bell-shaped, 68-95-99.7 rule"
    Standard Normal: "Œº = 0, œÉ = 1, denoted as Z ~ N(0,1)"
    Implementation: "scipy.stats.norm"
    
  Student's t-Distribution:
    Parameter: "ŒΩ (degrees of freedom)"
    PDF: "Complex gamma function formula"
    Properties: "Symmetric, heavier tails than normal, approaches normal as ŒΩ ‚Üí ‚àû"
    Use Cases: "Small sample inference, t-tests, confidence intervals"
    Implementation: "scipy.stats.t"
    
  Chi-Square Distribution:
    Parameter: "k (degrees of freedom)"
    PDF: "f(x) = (1/2^(k/2)Œì(k/2)) √ó x^(k/2-1) √ó e^(-x/2)"
    Mean: "k"
    Variance: "2k"
    Use Cases: "Goodness of fit, independence tests, variance estimation"
    Implementation: "scipy.stats.chi2"
    
  F-Distribution:
    Parameters: "d‚ÇÅ, d‚ÇÇ (degrees of freedom)"
    PDF: "Complex beta function formula"
    Use Cases: "ANOVA, regression F-tests, variance ratio tests"
    Implementation: "scipy.stats.f"
    
  Exponential Distribution:
    Parameter: "Œª (rate parameter)"
    PDF: "f(x) = Œªe^(-Œªx) for x ‚â• 0"
    Mean: "1/Œª"
    Variance: "1/Œª¬≤"
    Use Cases: "Survival analysis, reliability, waiting times"
    Implementation: "scipy.stats.expon"
    
  Beta Distribution:
    Parameters: "Œ±, Œ≤ (shape parameters)"
    PDF: "f(x) = x^(Œ±-1)(1-x)^(Œ≤-1) / B(Œ±,Œ≤) for 0 < x < 1"
    Mean: "Œ±/(Œ±+Œ≤)"
    Variance: "Œ±Œ≤/((Œ±+Œ≤)¬≤(Œ±+Œ≤+1))"
    Use Cases: "Bayesian priors, proportions, rates"
    Implementation: "scipy.stats.beta"
```

### **Statistical Inference and Hypothesis Testing**

#### **Parameter Estimation**
```yaml
Point Estimation:
  Method of Moments:
    Concept: "Equate sample moments to population moments"
    Procedure: "Set sample mean = population mean, sample variance = population variance"
    Advantages: "Simple, intuitive"
    Disadvantages: "Not always efficient, may not exist"
    
  Maximum Likelihood Estimation (MLE):
    Concept: "Find parameters that maximize likelihood function"
    Likelihood Function: "L(Œ∏) = ‚àè f(x·µ¢; Œ∏)"
    Log-Likelihood: "‚Ñì(Œ∏) = Œ£ log f(x·µ¢; Œ∏)"
    Properties: "Asymptotically unbiased, efficient, normal"
    
    Implementation Example:
      from scipy.optimize import minimize_scalar
      from scipy.stats import norm
      
      def neg_log_likelihood(params, data):
          mu, sigma = params
          return -np.sum(norm.logpdf(data, mu, sigma))
          
  Least Squares Estimation:
    Ordinary Least Squares: "Minimize Œ£(y·µ¢ - ≈∑·µ¢)¬≤"
    Weighted Least Squares: "Account for heteroscedasticity"
    Generalized Least Squares: "Correlated errors"
    
    Properties:
      BLUE: "Best Linear Unbiased Estimator under Gauss-Markov assumptions"
      Assumptions: "Linear relationship, independence, homoscedasticity, normality"

Interval Estimation:
  Confidence Intervals:
    Definition: "Interval that contains true parameter with specified probability"
    Interpretation: "If we repeat sampling, Œ±% of intervals will contain true parameter"
    
    For Mean (œÉ known): "xÃÑ ¬± z_{Œ±/2} √ó (œÉ/‚àön)"
    For Mean (œÉ unknown): "xÃÑ ¬± t_{Œ±/2,n-1} √ó (s/‚àön)"
    For Proportion: "pÃÇ ¬± z_{Œ±/2} √ó ‚àö(pÃÇ(1-pÃÇ)/n)"
    For Variance: "((n-1)s¬≤/œá¬≤_{Œ±/2,n-1}, (n-1)s¬≤/œá¬≤_{1-Œ±/2,n-1})"
    
    Implementation:
      from scipy import stats
      confidence_interval = stats.t.interval(confidence, df, loc=sample_mean, scale=std_error)

Bootstrap Methods:
  Non-parametric Bootstrap:
    Procedure: "Resample with replacement, calculate statistic, repeat B times"
    Bootstrap Distribution: "Approximates sampling distribution of statistic"
    Confidence Intervals: "Percentile method, bias-corrected and accelerated (BCa)"
    
    Implementation:
      import numpy as np
      from sklearn.utils import resample
      
      bootstrap_samples = []
      for i in range(n_bootstrap):
          sample = resample(data)
          bootstrap_samples.append(statistic_function(sample))
```

#### **Hypothesis Testing Framework**
```yaml
Hypothesis Testing Components:
  Null Hypothesis (H‚ÇÄ): "Statement of no effect or no difference"
  Alternative Hypothesis (H‚ÇÅ): "What we want to establish"
  
  Types of Alternatives:
    Two-tailed: "H‚ÇÅ: Œº ‚â† Œº‚ÇÄ"
    Upper-tailed: "H‚ÇÅ: Œº > Œº‚ÇÄ"
    Lower-tailed: "H‚ÇÅ: Œº < Œº‚ÇÄ"
    
  Test Statistic: "Function of sample data used for decision"
  P-value: "Probability of observing test statistic as extreme as observed, given H‚ÇÄ is true"
  Significance Level (Œ±): "Probability of Type I error (rejecting true H‚ÇÄ)"
  
  Decision Rule:
    "If p-value ‚â§ Œ±, reject H‚ÇÄ"
    "If p-value > Œ±, fail to reject H‚ÇÄ"

Error Types:
  Type I Error: "Reject H‚ÇÄ when H‚ÇÄ is true (false positive)"
  Type II Error: "Fail to reject H‚ÇÄ when H‚ÇÅ is true (false negative)"
  Power: "1 - Œ≤ = P(reject H‚ÇÄ | H‚ÇÅ is true)"
  
  Factors Affecting Power:
    "Effect size (larger = more power)"
    "Sample size (larger = more power)"
    "Significance level (larger = more power)"
    "Population variability (smaller = more power)"

Common Test Statistics:
  One-sample t-test:
    Statistic: "t = (xÃÑ - Œº‚ÇÄ) / (s/‚àön)"
    Distribution: "t_{n-1} under H‚ÇÄ"
    Assumptions: "Normal population or large sample"
    
  Two-sample t-test:
    Equal Variances: "t = (xÃÑ‚ÇÅ - xÃÑ‚ÇÇ) / (s_p‚àö(1/n‚ÇÅ + 1/n‚ÇÇ))"
    Unequal Variances (Welch): "t = (xÃÑ‚ÇÅ - xÃÑ‚ÇÇ) / ‚àö(s‚ÇÅ¬≤/n‚ÇÅ + s‚ÇÇ¬≤/n‚ÇÇ)"
    Degrees of Freedom: "Satterthwaite approximation for unequal variances"
    
  Paired t-test:
    Statistic: "t = dÃÑ / (s_d/‚àön)"
    Where: "dÃÑ = mean of differences, s_d = standard deviation of differences"
    
  Chi-square Goodness of Fit:
    Statistic: "œá¬≤ = Œ£ (Observed - Expected)¬≤ / Expected"
    Distribution: "œá¬≤_{k-1} where k = number of categories"
    
  Chi-square Test of Independence:
    Statistic: "œá¬≤ = Œ£ (O_{ij} - E_{ij})¬≤ / E_{ij}"
    Distribution: "œá¬≤_{(r-1)(c-1)} where r = rows, c = columns"
```

### **Design of Experiments (DOE)**

#### **Experimental Design Principles**
```yaml
Fundamental Principles:
  Randomization:
    Purpose: "Eliminate bias, ensure validity of statistical inference"
    Types: "Simple randomization, block randomization, stratified randomization"
    Implementation: "Random assignment of treatments to experimental units"
    
  Replication:
    Purpose: "Estimate experimental error, increase precision"
    Types: "True replication vs pseudoreplication"
    Benefits: "Increased power, better error estimation"
    
  Blocking:
    Purpose: "Control for known sources of variation"
    Principle: "Group similar experimental units together"
    Benefits: "Reduced experimental error, increased precision"

Experimental Units vs Observational Units:
  Experimental Unit: "Smallest unit to which treatment is independently applied"
  Observational Unit: "Unit on which response is measured"
  Pseudoreplication: "Treating observational units as if they were experimental units"

Control and Treatment Structure:
  Control Groups:
    Negative Control: "No treatment"
    Positive Control: "Known effective treatment"
    Placebo Control: "Inactive treatment to control for psychological effects"
    
  Treatment Factors:
    Fixed Effects: "Treatments chosen by experimenter"
    Random Effects: "Treatments randomly sampled from population"
    Mixed Effects: "Combination of fixed and random effects"
```

#### **Common Experimental Designs**
```yaml
Completely Randomized Design (CRD):
  Structure: "Treatments assigned completely at random to experimental units"
  Model: "y_{ij} = Œº + œÑ_i + Œµ_{ij}"
  Advantages: "Simple, flexible, maximum degrees of freedom for error"
  Disadvantages: "No control for extraneous variation"
  
  Analysis:
    One-way ANOVA: "Test H‚ÇÄ: œÑ‚ÇÅ = œÑ‚ÇÇ = ... = œÑ_t = 0"
    F-statistic: "F = MST/MSE ~ F_{t-1, t(n-1)}"
    
  Implementation:
    from scipy import stats
    f_stat, p_value = stats.f_oneway(group1, group2, group3, ...)

Randomized Complete Block Design (RCBD):
  Structure: "Each block contains all treatments, randomized within blocks"
  Model: "y_{ij} = Œº + œÑ_i + Œ≤_j + Œµ_{ij}"
  Advantages: "Controls for block effects, increased precision"
  
  Analysis:
    Two-way ANOVA: "Test effects of treatments and blocks"
    Efficiency: "Compare to CRD using relative efficiency"
    
  Implementation:
    import statsmodels.api as sm
    from statsmodels.formula.api import ols
    
    model = ols('response ~ C(treatment) + C(block)', data=df).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)

Latin Square Design:
  Structure: "Two blocking factors, each treatment appears once in each row and column"
  Model: "y_{ijk} = Œº + œÑ_i + œÅ_j + Œ≥_k + Œµ_{ijk}"
  Restrictions: "Number of treatments = number of rows = number of columns"
  
  Advantages: "Controls for two sources of variation"
  Disadvantages: "Restrictive, assumes no interactions"

Factorial Designs:
  2^k Design:
    Structure: "k factors, each at 2 levels"
    Treatments: "2^k treatment combinations"
    Effects: "Main effects and interactions"
    
  Main Effects:
    Factor A: "A = (sum of responses at high level - sum at low level) / (n √ó 2^{k-1})"
    
  Interaction Effects:
    AB Interaction: "AB = (sum of responses where A and B have same sign - sum where different sign) / (n √ó 2^{k-1})"
    
  Fractional Factorial:
    Purpose: "Reduce number of runs while maintaining information on important effects"
    Resolution: "Ability to estimate effects free of aliasing with lower-order effects"
    
Response Surface Methodology (RSM):
  Purpose: "Optimize response by finding optimal factor settings"
  Central Composite Design: "Factorial points + axial points + center points"
  Box-Behnken Design: "Three-level design, no axial points"
  
  Model: "y = Œ≤‚ÇÄ + Œ£Œ≤·µ¢x·µ¢ + Œ£Œ≤·µ¢·µ¢x·µ¢¬≤ + Œ£Œ£Œ≤·µ¢‚±ºx·µ¢x‚±º + Œµ"
  Analysis: "Canonical analysis to find stationary point"
```

### **Regression Analysis Mastery**

#### **Simple Linear Regression**
```yaml
Model Specification:
  Population Model: "Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ"
  Assumptions: "Œµ ~ N(0, œÉ¬≤), independence, linearity, homoscedasticity"
  
Parameter Estimation:
  Least Squares Estimators:
    Slope: "Œ≤ÃÇ‚ÇÅ = Œ£(x·µ¢ - xÃÑ)(y·µ¢ - »≥) / Œ£(x·µ¢ - xÃÑ)¬≤"
    Intercept: "Œ≤ÃÇ‚ÇÄ = »≥ - Œ≤ÃÇ‚ÇÅxÃÑ"
    
  Properties:
    Unbiased: "E[Œ≤ÃÇ‚ÇÄ] = Œ≤‚ÇÄ, E[Œ≤ÃÇ‚ÇÅ] = Œ≤‚ÇÅ"
    Minimum Variance: "BLUE under Gauss-Markov assumptions"
    
Inference:
  Standard Errors:
    SE(Œ≤ÃÇ‚ÇÅ): "œÉÃÇ‚àö(1/Œ£(x·µ¢ - xÃÑ)¬≤)"
    SE(Œ≤ÃÇ‚ÇÄ): "œÉÃÇ‚àö(1/n + xÃÑ¬≤/Œ£(x·µ¢ - xÃÑ)¬≤)"
    
  Hypothesis Tests:
    t-test for slope: "t = Œ≤ÃÇ‚ÇÅ / SE(Œ≤ÃÇ‚ÇÅ) ~ t_{n-2}"
    F-test for regression: "F = MSR/MSE ~ F_{1,n-2}"
    
  Confidence and Prediction Intervals:
    CI for mean response: "≈∑ ¬± t_{Œ±/2,n-2} √ó SE(≈∑)"
    PI for individual response: "≈∑ ¬± t_{Œ±/2,n-2} √ó ‚àö(MSE + SE¬≤(≈∑))"

Model Diagnostics:
  Residual Analysis:
    Residuals: "e·µ¢ = y·µ¢ - ≈∑·µ¢"
    Standardized residuals: "r·µ¢ = e·µ¢ / ‚àöMSE"
    Studentized residuals: "t·µ¢ = e·µ¢ / (s‚àö(1 - h·µ¢·µ¢))"
    
  Assumption Checking:
    Linearity: "Residuals vs fitted values plot"
    Normality: "Q-Q plot, Shapiro-Wilk test"
    Homoscedasticity: "Residuals vs fitted, Breusch-Pagan test"
    Independence: "Residuals vs time order, Durbin-Watson test"
```

#### **Multiple Linear Regression**
```yaml
Model Specification:
  Matrix Form: "Y = XŒ≤ + Œµ"
  Where: "Y is n√ó1, X is n√ó(p+1), Œ≤ is (p+1)√ó1, Œµ is n√ó1"
  
Parameter Estimation:
  Normal Equations: "(X'X)Œ≤ÃÇ = X'Y"
  Least Squares Solution: "Œ≤ÃÇ = (X'X)‚Åª¬πX'Y"
  Fitted Values: "≈∂ = XŒ≤ÃÇ = X(X'X)‚Åª¬πX'Y = HY"
  Hat Matrix: "H = X(X'X)‚Åª¬πX'"
  
Inference:
  Variance-Covariance Matrix: "Var(Œ≤ÃÇ) = œÉ¬≤(X'X)‚Åª¬π"
  Standard Errors: "SE(Œ≤ÃÇ‚±º) = œÉÃÇ‚àö((X'X)‚Åª¬π)‚±º‚±º"
  
  t-tests for Individual Coefficients:
    Test: "H‚ÇÄ: Œ≤‚±º = 0 vs H‚ÇÅ: Œ≤‚±º ‚â† 0"
    Statistic: "t = Œ≤ÃÇ‚±º / SE(Œ≤ÃÇ‚±º) ~ t_{n-p-1}"
    
  F-test for Overall Regression:
    Test: "H‚ÇÄ: Œ≤‚ÇÅ = Œ≤‚ÇÇ = ... = Œ≤‚Çö = 0"
    Statistic: "F = MSR/MSE = (SSR/p)/(SSE/(n-p-1)) ~ F_{p,n-p-1}"

Model Selection:
  Criteria:
    R¬≤: "Proportion of variance explained"
    Adjusted R¬≤: "R¬≤‚Çê = 1 - (1-R¬≤)(n-1)/(n-p-1)"
    AIC: "Akaike Information Criterion = n ln(SSE/n) + 2p"
    BIC: "Bayesian Information Criterion = n ln(SSE/n) + p ln(n)"
    
  Selection Methods:
    Forward Selection: "Start empty, add variables based on significance"
    Backward Elimination: "Start full, remove variables based on significance"
    Stepwise: "Combination of forward and backward"
    All Subsets: "Consider all possible models"

Multicollinearity:
  Detection:
    Correlation Matrix: "High pairwise correlations (|r| > 0.8)"
    Variance Inflation Factor: "VIF = 1/(1-R¬≤‚±º) where R¬≤‚±º is R¬≤ from regressing x‚±º on other predictors"
    Condition Number: "Œ∫ = ‚àö(Œª‚Çò‚Çê‚Çì/Œª‚Çò·µ¢‚Çô) where Œª are eigenvalues of X'X"
    
  Remedies:
    Ridge Regression: "Add penalty ŒªŒ£Œ≤‚±º¬≤ to minimize"
    Principal Components Regression: "Use principal components as predictors"
    Variable Selection: "Remove redundant variables"
```

#### **Logistic Regression**
```yaml
Model Specification:
  Logit Link: "logit(œÄ) = log(œÄ/(1-œÄ)) = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çöx‚Çö"
  Probability: "œÄ = exp(Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çöx‚Çö) / (1 + exp(Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çöx‚Çö))"
  
Maximum Likelihood Estimation:
  Likelihood: "L(Œ≤) = ‚àè œÄ·µ¢ ∏‚Å±(1-œÄ·µ¢)¬π‚Åª ∏‚Å±"
  Log-likelihood: "‚Ñì(Œ≤) = Œ£[y·µ¢ log(œÄ·µ¢) + (1-y·µ¢) log(1-œÄ·µ¢)]"
  Score Equations: "‚àÇ‚Ñì/‚àÇŒ≤ = X'(Y - œÄ) = 0"
  
Inference:
  Wald Tests:
    Statistic: "z = Œ≤ÃÇ‚±º / SE(Œ≤ÃÇ‚±º) ~ N(0,1) for large samples"
    Confidence Interval: "Œ≤ÃÇ‚±º ¬± z_{Œ±/2} √ó SE(Œ≤ÃÇ‚±º)"
    
  Likelihood Ratio Tests:
    Statistic: "G¬≤ = -2 log(L‚ÇÄ/L‚ÇÅ) = -2[‚Ñì(Œ≤ÃÇ‚ÇÄ) - ‚Ñì(Œ≤ÃÇ‚ÇÅ)] ~ œá¬≤_{df}"
    Where: "df = difference in number of parameters"
    
Interpretation:
  Odds Ratio: "OR = exp(Œ≤‚±º)"
  Interpretation: "For one unit increase in x‚±º, odds multiply by exp(Œ≤‚±º)"
  
Model Diagnostics:
  Deviance: "D = -2‚Ñì(Œ≤ÃÇ)"
  Hosmer-Lemeshow Test: "Goodness of fit test"
  ROC Curve: "Receiver Operating Characteristic"
  C-statistic: "Area under ROC curve"
  
Implementation:
  import statsmodels.api as sm
  from sklearn.linear_model import LogisticRegression
  
  # Statsmodels approach
  model = sm.Logit(y, X).fit()
  
  # Sklearn approach
  model = LogisticRegression().fit(X, y)
```

### **Analysis of Variance (ANOVA)**

#### **One-Way ANOVA**
```yaml
Model and Assumptions:
  Model: "y·µ¢‚±º = Œº + œÑ·µ¢ + Œµ·µ¢‚±º"
  Where: "Œº = overall mean, œÑ·µ¢ = treatment effect, Œµ·µ¢‚±º ~ N(0, œÉ¬≤)"
  
  Assumptions:
    Independence: "Observations within and between groups are independent"
    Normality: "Errors are normally distributed"
    Homogeneity: "Equal variances across groups (homoscedasticity)"
    
Hypothesis Testing:
  Hypotheses: "H‚ÇÄ: Œº‚ÇÅ = Œº‚ÇÇ = ... = Œº‚Çñ vs H‚ÇÅ: Not all means are equal"
  Equivalent: "H‚ÇÄ: œÑ‚ÇÅ = œÑ‚ÇÇ = ... = œÑ‚Çñ = 0"
  
ANOVA Table:
  Source | SS | df | MS | F
  Treatment | SST | k-1 | MST = SST/(k-1) | F = MST/MSE
  Error | SSE | n-k | MSE = SSE/(n-k) |
  Total | SSTO | n-1 | |
  
  Where:
    SST: "Œ£n·µ¢(xÃÑ·µ¢ - xÃÑ..)¬≤"
    SSE: "Œ£Œ£(x·µ¢‚±º - xÃÑ·µ¢)¬≤"
    SSTO: "Œ£Œ£(x·µ¢‚±º - xÃÑ..)¬≤"
    
F-test:
  Test Statistic: "F = MST/MSE ~ F_{k-1, n-k} under H‚ÇÄ"
  Decision: "Reject H‚ÇÄ if F > F_{Œ±, k-1, n-k}"

Multiple Comparisons:
  Family-wise Error Rate: "Probability of making at least one Type I error"
  
  Bonferroni Correction:
    Adjusted Œ±: "Œ±' = Œ±/m where m = number of comparisons"
    Conservative but widely applicable
    
  Tukey's HSD (Honestly Significant Difference):
    For pairwise comparisons: "HSD = q_{Œ±,k,n-k} √ó ‚àö(MSE/n)"
    Where: "q is studentized range statistic"
    Controls family-wise error rate exactly
    
  Scheffe's Method:
    Most conservative: "Works for any contrast"
    Critical value: "‚àö((k-1)F_{Œ±,k-1,n-k})"
    
  Dunnett's Test:
    Comparing treatments to control: "Uses special tables"
    More powerful than Bonferroni for this specific case
```

#### **Two-Way ANOVA**
```yaml
Model Specifications:
  Additive Model: "y·µ¢‚±º‚Çñ = Œº + Œ±·µ¢ + Œ≤‚±º + Œµ·µ¢‚±º‚Çñ"
  Interaction Model: "y·µ¢‚±º‚Çñ = Œº + Œ±·µ¢ + Œ≤‚±º + (Œ±Œ≤)·µ¢‚±º + Œµ·µ¢‚±º‚Çñ"
  
  Where:
    Œ±·µ¢: "Effect of factor A at level i"
    Œ≤‚±º: "Effect of factor B at level j"
    (Œ±Œ≤)·µ¢‚±º: "Interaction effect between A and B"

Hypothesis Tests:
  Main Effect A: "H‚ÇÄ: Œ±‚ÇÅ = Œ±‚ÇÇ = ... = Œ±‚Çê = 0"
  Main Effect B: "H‚ÇÄ: Œ≤‚ÇÅ = Œ≤‚ÇÇ = ... = Œ≤·µ¶ = 0"
  Interaction: "H‚ÇÄ: (Œ±Œ≤)·µ¢‚±º = 0 for all i,j"

ANOVA Table (with interaction):
  Source | SS | df | MS | F
  A | SSA | a-1 | MSA | MSA/MSE
  B | SSB | b-1 | MSB | MSB/MSE
  A√óB | SSAB | (a-1)(b-1) | MSAB | MSAB/MSE
  Error | SSE | ab(n-1) | MSE |
  Total | SSTO | abn-1 | |

Interaction Interpretation:
  No Interaction: "Effects of factors are additive"
  Significant Interaction: "Effect of one factor depends on level of other factor"
  
  Simple Effects Analysis:
    Test effect of A at each level of B
    Test effect of B at each level of A
    Use MSE from full model for error term

Fixed vs Random Effects:
  Fixed Effects: "Factor levels chosen by experimenter"
  Random Effects: "Factor levels randomly sampled from population"
  Mixed Model: "Some factors fixed, some random"
  
  Random Effects Model:
    Different F-tests: "Use appropriate error terms"
    Variance Components: "Estimate variability due to each factor"
```

### **Survey Sampling and Design**

#### **Sampling Theory Foundations**
```yaml
Basic Sampling Concepts:
  Population: "Complete set of units of interest"
  Sampling Frame: "List of units from which sample is selected"
  Sample: "Subset of population selected for study"
  Sampling Unit: "Basic unit selected in sampling process"
  
Coverage and Non-coverage:
  Coverage Error: "Difference between target population and sampling frame"
  Under-coverage: "Frame excludes some target population units"
  Over-coverage: "Frame includes non-target population units"
  
Probability Sampling:
  Definition: "Each unit has known, non-zero probability of selection"
  Advantages: "Unbiased estimation, measurable precision"
  Selection Probability: "œÄ·µ¢ = P(unit i is selected)"
  
Non-probability Sampling:
  Convenience Sampling: "Select easily accessible units"
  Purposive Sampling: "Select units based on judgment"
  Quota Sampling: "Select specified number from each stratum"
  Limitations: "Potential bias, unmeasurable precision"
```

#### **Simple Random Sampling (SRS)**
```yaml
Without Replacement (WOR):
  Selection: "Each unit has equal probability n/N of selection"
  Sample Mean: "»≥ = Œ£y·µ¢/n"
  Population Total Estimation: "TÃÇ = N»≥"
  
Variance Estimation:
  Population Mean: "Var(»≥) = (1 - n/N) √ó S¬≤/n"
  Population Total: "Var(TÃÇ) = N¬≤(1 - n/N) √ó S¬≤/n"
  
  Where S¬≤: "S¬≤ = Œ£(y·µ¢ - »≤)¬≤/(N-1)"
  
Finite Population Correction:
  FPC: "(1 - n/N) = (N - n)/N"
  Applied when: "Sampling fraction n/N > 0.05"
  Effect: "Reduces variance when sample is large relative to population"

Sample Size Determination:
  For Estimating Mean:
    Desired Margin of Error: "d = z_{Œ±/2} √ó ‚àö(Var(»≥))"
    Required Sample Size: "n = z¬≤_{Œ±/2} √ó S¬≤ / (d¬≤ + z¬≤_{Œ±/2} √ó S¬≤/N)"
    
  For Estimating Proportion:
    Conservative Approach: "n = z¬≤_{Œ±/2} √ó 0.25 / d¬≤"
    With Prior Estimate: "n = z¬≤_{Œ±/2} √ó p(1-p) / d¬≤"

Implementation:
  import numpy as np
  from scipy import stats
  
  def srs_estimate(sample, N):
      n = len(sample)
      y_bar = np.mean(sample)
      s_squared = np.var(sample, ddof=1)
      
      # Population mean estimate
      mu_hat = y_bar
      
      # Variance of mean estimate
      fpc = (N - n) / N if n/N > 0.05 else 1
      var_mu_hat = fpc * s_squared / n
      
      return mu_hat, var_mu_hat
```

#### **Stratified Sampling**
```yaml
Design Structure:
  Population Division: "Divide population into L strata"
  Within-stratum Sampling: "Select sample from each stratum"
  Stratum Sample Size: "n‚Çï units from stratum h"
  
Allocation Methods:
  Proportional Allocation:
    Formula: "n‚Çï = n √ó (N‚Çò/N)"
    Properties: "Self-weighting, simple analysis"
    
  Optimal Allocation:
    Formula: "n‚Çï = n √ó (N‚ÇïS‚Çï/Œ£(N‚ÇïS‚Çï))"
    Objective: "Minimize variance of overall estimate"
    Requires: "Prior knowledge of stratum variances"
    
  Neyman Allocation:
    Formula: "n‚Çï = n √ó (N‚ÇïS‚Çï/Œ£(N‚ÇïS‚Çï))"
    Same as optimal allocation
    
Estimation:
  Stratified Mean:
    »≥‚Çõ‚Çú = Œ£(N‚Çï/N) √ó »≥‚Çï = Œ£ W‚Çï √ó »≥‚Çï
    Where W‚Çï = N‚Çï/N (stratum weight)
    
  Variance:
    Var(»≥‚Çõ‚Çú) = Œ£ W‚Çï¬≤ √ó (1 - n‚Çï/N‚Çï) √ó S‚Çï¬≤/n‚Çï
    
Advantages:
  Precision: "Usually more precise than SRS"
  Subpopulation Analysis: "Direct estimates for strata"
  Administrative Convenience: "Separate sampling operations"
  
Design Effect:
  Definition: "Ratio of actual variance to SRS variance"
  DEFF = Var(»≥‚Çõ‚Çú) / Var(»≥‚Çõ·µ£‚Çõ)
  Values < 1 indicate stratification benefit
```

#### **Cluster Sampling**
```yaml
Design Structure:
  Primary Sampling Units: "Clusters (e.g., schools, households)"
  Secondary Sampling Units: "Elements within clusters (e.g., students, individuals)"
  
One-stage Cluster Sampling:
  Selection: "Select clusters, include all elements within selected clusters"
  Estimate: "»≥c = Œ£(M·µ¢»≥·µ¢)/Œ£ M·µ¢"
  Where: "M·µ¢ = number of elements in cluster i, »≥·µ¢ = cluster mean"
  
Two-stage Cluster Sampling:
  Stage 1: "Select primary clusters"
  Stage 2: "Select secondary units within selected clusters"
  
  Estimation:
    Cluster Total: "t·µ¢ = m·µ¢»≥·µ¢" where m·µ¢ = sample size in cluster i
    Overall Estimate: "»≥ = Œ£t·µ¢/Œ£m·µ¢"
    
Intracluster Correlation:
  Definition: "œÅ = Correlation between elements within same cluster"
  Effect: "œÅ > 0 increases variance (elements similar within clusters)"
  Design Effect: "DEFF ‚âà 1 + (mÃÑ - 1)œÅ" where mÃÑ = average cluster size
  
Variance Estimation:
  Between-cluster Component: "Reflects cluster-to-cluster variation"
  Within-cluster Component: "Reflects element-to-element variation within clusters"
  
  Combined Variance:
    V(»≥c) = (1 - n/N) √ó S¬≤bc/n + (1/n) √ó Œ£(1 - m·µ¢/M·µ¢) √ó S¬≤wc,i/m·µ¢
    
Advantages and Disadvantages:
  Advantages: "Cost efficiency, administrative convenience"
  Disadvantages: "Usually less precise than SRS, requires variance inflation"
```

---

## üéØ **Agent Implementation Guidance**

### **How This Technical Mastery Enhances Agent Performance**

#### **Statistical Research and Implementation Support**
- **Mathematical Foundation**: Deep understanding of statistical theory and mathematical principles
- **Method Selection**: Expert knowledge of when to apply specific statistical techniques
- **Implementation Guidance**: Comprehensive code examples and parameter specifications
- **Diagnostic Expertise**: Advanced knowledge of assumption checking and model validation

#### **Problem-Solving Approach**
- **Statistical Reasoning**: Ability to identify appropriate statistical methods for research questions
- **Experimental Design**: Expert knowledge of designing studies and experiments
- **Data Analysis**: Comprehensive understanding of analysis techniques and interpretation
- **Quality Assurance**: Expertise in validation, diagnostics, and robustness checking

### **Agent Usage Instructions**

#### **When to Apply This Technical Knowledge**
```python
# Example usage in agent decision-making
if research_question.type == "hypothesis_testing":
    determine_appropriate_test()
    check_assumptions_and_requirements()
    provide_implementation_guidance()
    
if study_design.type == "experimental":
    recommend_experimental_design()
    calculate_sample_size_requirements()
    suggest_randomization_strategy()
    
if data_characteristics.has_multiple_groups:
    recommend_anova_approach()
    suggest_multiple_comparison_procedures()
    provide_effect_size_calculations()
```

#### **Research Output Enhancement**
All Statistician agent research should include:
- **Mathematical foundations** with formulas and theoretical basis
- **Assumption checking procedures** with diagnostic tests and validation methods
- **Implementation code** with specific parameters and statistical software usage
- **Interpretation guidelines** with effect sizes and practical significance
- **Alternative approaches** when assumptions are violated or methods are inappropriate

---

*This technical mastery knowledge base transforms the Statistician Agent from general guidance to deep statistical expertise, enabling sophisticated research and implementation recommendations for statistical analysis, experimental design, and mathematical modeling challenges.*

**¬© 2025 Fed Job Advisor - Statistician Agent Technical Mastery Enhancement**